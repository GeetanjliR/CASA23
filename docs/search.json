[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learning Dairy",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "WeekOne.html",
    "href": "WeekOne.html",
    "title": "2  Week One: Introduction and Background",
    "section": "",
    "text": "3 Remote Sensing Cities and Environment\nRecent trend on studies around urban area, green space, access to infrastructure, impact on health & well-being has brought the rise of remote sensing cities and environment. NASA defines remote sensing as acquiring information from a distance, by sensors that are mounted on satellites, planes (aerial imagery), drones, phones, free standing on the ground or sea (with hand held devices) etc. (CASA0023).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week One: Introduction and Background</span>"
    ]
  },
  {
    "objectID": "WeekTwo.html",
    "href": "WeekTwo.html",
    "title": "3  Week Two",
    "section": "",
    "text": "3.1 Reference\nHow use tools as assessment For learning dairy: 1. Xaringan 2. Quarto (for individual) -next week hw to take a Sensor of choice and make a ppt on Xaringan Quorto is platform independent to be done next week — triple dash slide here ??? triple ques mark: any ? comments/ presenter view –Double dash break slide/ Pull-left/ pull-right — new slide Image folder [] hyperlink () simple bracket Make csv/ excel/ google sheet and load in R Citing: Zotero must be updated and better works with bibltx CSS: theme",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week Two</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "WeekTwo.html#quarto",
    "href": "WeekTwo.html#quarto",
    "title": "3  Week Two",
    "section": "3.2 Quarto",
    "text": "3.2 Quarto\nBookdown was used earlier was compatible with R but not with other programming language. Then came along quarto with different chapters in different markdown. use # for new slide Youtube- share-imbed Index.rmd-don’t change HW: 5-9 slides on sensor put on git/ link in excel shared No code on quarto just text Use kable for making tables reference will be learnt in week 3 use eval = false when a code need not be read to un-render a chapter/ file use ’_’ underscore in the beginning eg _quarto.yml file in the output dir docs setup a quarto dairy for this week go back to last edit Go to Book, Render, Add, commit, push Do render html and not pdf",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week Two</span>"
    ]
  },
  {
    "objectID": "WeekOne.html#satellite",
    "href": "WeekOne.html#satellite",
    "title": "2  Week One: Introduction and Background",
    "section": "3.1 Satellite",
    "text": "3.1 Satellite\nNASA launched Landsat satellites that changed the method of collecting earth’s landscape data popularly known as remote sensing which interchangeably called as ‘Earth Observation’. Scientist started to get more data and details as the NASA went ahead with launches of Landsat1 to Landsat8. So far to receive enormous detailed data about earth’s resources and climate, the best has been Landsat8, With more sensitive sensors scientist can retrieve improved accuracy in data that helps to manage the earth’s resources and climate optimally. For example, study to most subtle change is vegetation is possible now with Landsat8.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week One: Introduction and Background</span>"
    ]
  },
  {
    "objectID": "WeekOne.html#sensors",
    "href": "WeekOne.html#sensors",
    "title": "2  Week One: Introduction and Background",
    "section": "3.2 Sensors",
    "text": "3.2 Sensors\nSensors are mounted on satellites, drones etc. The sensors monitor electromagnetic rays of spectrum. They are of two types\n\n3.2.1 Passive Sensors\nPassive sensors are those that does not emit energy but uses the energy that is available i.e., from the sun to reflects back energy. The energy reflected back is in electromagnetic waves. For example human eye, camera, satellite sensor.\n\n\n3.2.2 Active Sensors\nThey are those that emit energy. Have an energy source for illumination Actively emits electormagentic waves and then waits to receive Such as: Radar, X-ray, LiDAR Sensors can be mounted on any platform.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week One: Introduction and Background</span>"
    ]
  },
  {
    "objectID": "WeekOne.html#electromagnetic-spectrum",
    "href": "WeekOne.html#electromagnetic-spectrum",
    "title": "2  Week One: Introduction and Background",
    "section": "3.3 Electromagnetic Spectrum",
    "text": "3.3 Electromagnetic Spectrum\nIt is around us all the time, without which the world around us would not exist is the Electromagnetic Radiations. These waves spread across from a very short radiations of gamma rays, x-rays, ultra-violate rays, visible-light waves, infrared waves, micro waves, radio waves. It is collectively known as the electromagnetic Spectrum or EMS. Electromagnetic (EM) waves are energy waves that emits having both electrical and magnetic properties is a two dimensional (2D) property.\nEM waves have crests and troughs like ocean waves and the distance between two crests is known as wavelength, see figure below. While some wavelengths are very long and measured in meters, some are very short and measured in nano-meters. The number of wavelengths passed at a point in 1 second is known as frequency of the wave. One wave or cycle per second is known as a Hertz (Hz).\n\n\n\n\n\n\n\n\n\nSource:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week One: Introduction and Background</span>"
    ]
  },
  {
    "objectID": "WeekOne.html#interacting-with-earths-surface",
    "href": "WeekOne.html#interacting-with-earths-surface",
    "title": "2  Week One: Introduction and Background",
    "section": "3.4 Interacting with Earth’s surface",
    "text": "3.4 Interacting with Earth’s surface\nBefore these radiation are retrieved by the sensors, the electromagnetic radiation(EMR) (e.g. from the sun) experiences surface and atmospheric changes. The surface changes are when energy gets absorbed by the surface energy being transmitted through the surface. There are below term important to understand: - Waves of an electromagnetic field, travel through space and carry radiant energy = Electromagnetic radiation (EMR). Waves are part of the EMR spectrum. - Energy carried by EMR waves = radiant energy -Energy per unit of time = radiant flux - Energy from the sun = incoming short wave radiation or shortwave radiation - Energy (solar power) from the sun per unit area per unit time (from electromagnetic radiation) = solar irradiance (per unit time - flux) - Energy leaving a surface per unit area per unit time = Exitance (emittance) (per unit time - flux) - Flux means time here.\nWhereas the atmospheric energy can be scattered by particles in the atmosphere. There are three types of atmospheric scattering: - Rayleigh = particles are very small compared to the wavelength - Mie = particles are the same size compared to the wavelength - Non selective = particles are much larger than the wavelength.\n\n3.4.1 SAR\nActive sensor such as Synthetic Aperture Radar (SAR) can see through clouds. Bidirectional Reflectance Distribution Function (BRDF) are surface interactions where view (e.g. sensor) and illumination (e.g. sun) angles can change. Energy being reflected from the surface that is smooth or diffuse. Thus, BRDF for flat surface, water surface, vegetation surface have got different texture of surface. SAR can also work on polaristion and fluoresence. Satellite (and aerial) sensors are affected by the atmospheric scattering which can be delt with atmospheric correction. These will be discussed in later weeks. The data cant be directly used as energy reflected from Earth to a sensor. Perhaps, it needs correction due to there many interactions that influence the data being created and we use.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week One: Introduction and Background</span>"
    ]
  },
  {
    "objectID": "WeekOne.html#data-format",
    "href": "WeekOne.html#data-format",
    "title": "2  Week One: Introduction and Background",
    "section": "3.5 Data Format",
    "text": "3.5 Data Format\nIn the majority of cases remotely sensed data is raster. Data format is generally geo.tif. Raster data are stacked vertically in B!, B2, B3 etc.(insert img). LiDAR data points has x,y with the z-dimension to collect attitude and 3D analysis. Data format are of following 4 types:\n\nSpatial Resolution\n\nSpatial resolution has the size of the raster grid per pixel (e.g. 20cm or 30m). One can compare the spatial resolution of 10 by 10 cm vs 1 by 1 km.\n\nSpectral Resolution\n\nSpectral Signature is unique signature given to material on earth or its combination shown through a graph . Sensors are designed to monitor specific range of EM spectrum called band. Different sensors will have different number of bands. Spectral Resolution has the number of bands it records data in. Spectral resolution 1 means Bands: range of spectrum, where more rich the band, will have more elements to identify. Spectral resolution means to observe part of spectrum in the window.\nThe 7-12 bands are known as multiple spectral bands where as 10-25 bands are hyper spectral bands which has continuous spectrum. It not always satellite can record the bands but ground too known as spectroradiometer which are pure white refernce panels that needs to be calibrated.\n\nRadiometric Resolution It identifies differences in light or reflectance, in practice this is the range of possible values. The ability of a sensor to identify and show small differences in energy. The higher the resolution, the more sensitive it is i.e., resolution of 8 bit can have 256 possible values where a resolution of 4 bit has 16 possible values. The more sensitive, the more components to show where as lower the radiometric resolution the lower the quality of the image and possibility to differentiate features.\nTemporal Resolution\n\nThis is the resolution means the time it revisits (e.g. daily, every 7 days, on demand). Its low resolution means it has a large pixel size (e.g. MODIS is 500m by 500m pixel).\n\n3.5.1 Other Considerations\ngeosynchronous orbit (GSO) if the satellite matches the Earth’s rotation or if geostationary orbit holds same position, usually only for communications but some sensors are geostationary.\nIn order to carry data collection for a research question the answers should be looked is about will dictate what sensor is most appropriate size of features, date range, revisit requirement, spectral sensitivity, cost.\nHeads-up for practical: Loading Landsat and Sentinel data, SNAP and R, regions of interest and plot.\n\n\n3.5.2 Electromagnetic Spectrum\nLandsat vs Sentinel B1, B2, B3 B4 (insert img) and graphs (inster img)\n\n\n3.5.3 Atmosphere Correction\n\n\n3.5.4 SNAP\nonly 3 bands Spectral feature space (insert img of scatter plot) then we do a tassel cap transformation to combine coefficient to simplify. Principle Component Analysis (PCA) is carried to simplify on dryness for urban, brightness for greeness, wetness for yellow. (insert img). Although there are various way to carry PCA that are readily available e.g., Normalized Difference in Vegetation Index (NDVI).\nNDVI = insert img there are also healthy green vegetation\nNDWI: Normalized Difference in Water Index NDSI: Normalized Difference in Soil Index NDBI: Normalized Difference in Built-up Index\nthe study area could be spread over two tiles (insert img) and different dates. Thus, we combine and take step by step process.\nHowever, in Google Earth Engine (GEE) take all the titles of different dates and calculates the median to calculate …..\nSNAP when open images\nhigh albido:\n10m or landsat 30 m (different spect./ scale) upscaling or downscaling resampling Machine Learning (ML) is used to learn land cover change Spectral Signature vs Spectral Libraries\nRegion of Interest:\n\nPIF: suedo invariant features\n\nw\n\n\ndownload\n\n\nunderstand bands end member analysis/ MESMA\n\n\nsatellite Segments",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week One: Introduction and Background</span>"
    ]
  },
  {
    "objectID": "About.html",
    "href": "About.html",
    "title": "1  About the Learner, the Module and the Porfolio",
    "section": "",
    "text": "1.1 The Learner\nSgh! I don’t know about myself and believe me you don’t want to know me either. I do have a name Geetanjli Rani, for sure I am not confused because as a keen learner, I am always evolving and don’t like labels and boxes. With the same zeal I wish to learn the module of CASA0023 on the Remote Sensing Cities and Environment. As mentioned earlier, me and you will see my journey of evolution to grasp the outcome and objectives of the opted module. All the best to ME!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>About the Learner, the Module and the Porfolio</span>"
    ]
  },
  {
    "objectID": "About.html#the-module",
    "href": "About.html#the-module",
    "title": "1  About the Learner, the Module and the Porfolio",
    "section": "1.2 The Module",
    "text": "1.2 The Module\nThis the module of CASA0023 about the Remote Sensing Cities and Environment. The lecture happens for and hour and the next hour is for drop in session followed by practical scheduled on the other day. Assignment would comprise of two parts. First group submission to present Xaringan power point in week 10 that holds 30 percent of weightage. The second submission of 70 percentage weightage is about the learning dairy in Quarto which includes week wise notes into sections/ 4 paragraphs of summary, application and reflection with references. The learning dairy is an informal notes writing with diagrams and mind-mapping with a word limit of 4000 and 2 pages a week. Few of the best submissions samples are available from last year on the slack channel CASA0023. In the interest of the module, the Remote Sensing of the Environment, An Earth Resource Perspective by Jensen and Google Earth Engine handbook are the two important notebooks. The aims of this module will enable to operationalise remotely sensed Earth observation data for informing decisions on environmental hazards arising from a changing climate, specifically in relation to (a) urban areas and (b) future urban sustainability. Additionally, one could use the remotely sensed (RS) data and analysis to bring policy about managing earth’s resources and address climate change.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>About the Learner, the Module and the Porfolio</span>"
    ]
  },
  {
    "objectID": "About.html#the-portfolio",
    "href": "About.html#the-portfolio",
    "title": "1  About the Learner, the Module and the Porfolio",
    "section": "1.3 The Portfolio",
    "text": "1.3 The Portfolio\nIn order to understand my portfolio, one needs to get in my head. Thus, I will take you through a series of cool abbreviations, dictionary/glossary and figures introduced to me by our very intuitive Prof Andrew Maclachlan whom we call Prof Andy. Although, some the glossary/terms looked new for me while most of them were known, I am re-brushing my knowledge and kind of putting the jigsaw-puzzle-pieces together towards self challenge to the quest of learning the Remote Sensing Cities and Environment. These method of data collecting by remotely sensed devices are very fascinating to me that I have made an attempt to design this portfolio as personal journal of experience learning CASA0023. Thus, one may see the sections of summary, application and reflection. When arrive at the reflection part, one may witness my personnel experience in terms of my thoughts, ideas, struggles etc. Well! I am having fun designing my portfolio, I hope you feel have the same!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>About the Learner, the Module and the Porfolio</span>"
    ]
  },
  {
    "objectID": "About.html#learning-outcomes",
    "href": "About.html#learning-outcomes",
    "title": "1  About the Learner, the Module and the Porfolio",
    "section": "1.4 Learning Outcomes",
    "text": "1.4 Learning Outcomes\nOld Terms New Terms: Spectral Signature",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>About the Learner, the Module and the Porfolio</span>"
    ]
  },
  {
    "objectID": "WeekThree.html",
    "href": "WeekThree.html",
    "title": "5  Week Three: Remote Sensing Data and Corrections",
    "section": "",
    "text": "5.1 Corrections\nThe satellite imagery holds error from a variety of sources. These error could be from the sensor, the atmosphere, the terrain etc. The scan line correction (SLC) generated for a Whisk broom or spotlight or across track scanners to align the images. These could be possible via the method of regression by adding all of the vertical differences between the blue line and all of the residuals, it should sum to 0. Thus, they must be corrected as may be appropriate. In order to do that, the use of the imagery must be contextualize as discussed below.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week Three: Remote Sensing Data and Corrections</span>"
    ]
  },
  {
    "objectID": "WeekThree.html#corrections",
    "href": "WeekThree.html#corrections",
    "title": "5  Week Three: Remote Sensing Data and Corrections",
    "section": "",
    "text": "5.1.1 Geometric Correction\nDepending upon the given coordinate reference system, the remotely sensed data has collected image distortions can be introduced due to the view angle (off-nadir*), topography (e.g. hills not flat ground), wind (if from a plane), rotation of the earth (from satellite).\nTp deal with geometric distortions, Ground Control Points (GPS) are identified to match known points in the image and a reference dataset such as a local map, another image, GPS data from handheld device etc. The coordinates are modeled to give geometric transformation coefficients through the method of linear regression with the distorted x or y as the dependent or independent*. This is the plotted to minimise the RMSE where Jensen sets a RMSE value of 0.5. A slight shift in the data is the required. So, re-sampling the final raster by method of nearest neighbor, linear, cubic, cubic spline is done. Thus, the input grid to output grid to re-sampling.\n\n\n5.1.2 Atmospheric Correction\nIdeally there are two most important sources of environmental attenuation, the atmospheric scattering and topographic attenuation. Types of particle scattering (insert image) for Relative (to something) is done in two ways, the DOS and PIF. The Dark object subtraction (DOS) or histogram adjustment, searches each band for the darkest value then subtracts that from each pixel. Where as psuedo-invariant features (PIFs), assumes brightness pixels linearly related to a base image and applies regression per band, Then adjust the image based on the regression result. It needs to be noticed that y is the value of the base. So, to get y we multiply our new date pixel (x) by the coefficient and add the intercept value. Similarly, it is applies to the rest of the pixels.\nAbsolute (definitive) Correction requires to change digital brightness values into scaled surface reflectance. Thereafter, compare these scaled surface reflectance values across the planet. It can be done through atmospheric radiative transfer models from various options. The atmopshierc radiative transfer code such as MODTRAN 4+ and the Second Simulation of the Satellite Signal in the Solar Spectrum (6S) gives the scattering and absorption information which can now be used through python - called Py6S.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week Three: Remote Sensing Data and Corrections</span>"
    ]
  },
  {
    "objectID": "WeekThree.html#emprical-line-correction",
    "href": "WeekThree.html#emprical-line-correction",
    "title": "5  Week Three: Remote Sensing Data and Corrections",
    "section": "5.2 Emprical Line Correction",
    "text": "5.2 Emprical Line Correction\nThe in-situ measurements taken using a field spectrometer that requires measurements at the same time as the satellite overpass. Next, applying the linear regression to the measurements against the satellite data raw digital number in the following equation:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week Three: Remote Sensing Data and Corrections</span>"
    ]
  },
  {
    "objectID": "WeekThree.html#orthorectification-correction-topographic-correction",
    "href": "WeekThree.html#orthorectification-correction-topographic-correction",
    "title": "5  Week Three: Remote Sensing Data and Corrections",
    "section": "5.3 Orthorectification correction / topographic correction",
    "text": "5.3 Orthorectification correction / topographic correction\nThe view taken at an angle can create such image distortion where georectification is giving coordinates to an image and orthorectification is removing distortions. Thus, making the pixels viewed at nadir is a straight down view. It requires sensor geometry and an elevation model and corrected through the Software / formulas of Jensen as below:\nVarious softwares such as QGIS, SAGA GIS, R package topocorr, R package RStoolbox etc. for topographic corrections where atmospheric corrections needs to be done before this.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week Three: Remote Sensing Data and Corrections</span>"
    ]
  },
  {
    "objectID": "WeekThree.html#radiometric-calibration",
    "href": "WeekThree.html#radiometric-calibration",
    "title": "5  Week Three: Remote Sensing Data and Corrections",
    "section": "5.4 Radiometric Calibration",
    "text": "5.4 Radiometric Calibration\nPre-calibration (below formula) is required in a lab before a sensor is launched, it is then uses these measurements to adjust the data from the sensor. The image brightness and distribution as a Digital Number (or DN) gets captured by the sensors, converted to spectral radiance is the radiometric calibration (in below formula).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week Three: Remote Sensing Data and Corrections</span>"
    ]
  },
  {
    "objectID": "WeekThree.html#landsat-ard-surface-reflectance",
    "href": "WeekThree.html#landsat-ard-surface-reflectance",
    "title": "5  Week Three: Remote Sensing Data and Corrections",
    "section": "5.5 Landsat ARD & surface reflectance",
    "text": "5.5 Landsat ARD & surface reflectance\nLandsat ARD: already validated level 2 product that provides corrected images for data processing. The Landsat Ecosystem Disturbance Adaptive Processing System (LEDPAS) and the Landsat 8 Surface Reflectance algorithm (L8SR) could make this possible. Noted be about the Level 2 product means something has changed as may come across not a ARD, e.g. very high resolution, drone.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week Three: Remote Sensing Data and Corrections</span>"
    ]
  },
  {
    "objectID": "WeekThree.html#joining-data-sets",
    "href": "WeekThree.html#joining-data-sets",
    "title": "5  Week Three: Remote Sensing Data and Corrections",
    "section": "5.6 Joining data sets",
    "text": "5.6 Joining data sets\nTo creates a seamless mosaic or image(s) the feather of images together is carried in Remote Sensing. The seamline is dividing line with a base image and “other” or second image with 20-30 percent overlap a histogram is extracted. Next, a histogram matching algorithm is performed which gives similar brightness values of the two images and feathering is conducted.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week Three: Remote Sensing Data and Corrections</span>"
    ]
  },
  {
    "objectID": "WeekThree.html#enhancements",
    "href": "WeekThree.html#enhancements",
    "title": "5  Week Three: Remote Sensing Data and Corrections",
    "section": "5.7 Enhancements",
    "text": "5.7 Enhancements\nImagery can be “improved/enhanced” based on the energy reflected and the contrast between features But… How do these methods help in urban environments Does adding complexity to imagery (or creating new datasets) assist us in our aim?\nIt is contrasting images Other enhancements: NDVI, NBR Local enhancements: Edge enhancements: embossing, filter for example\nOriginal - low pass - High pass",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week Three: Remote Sensing Data and Corrections</span>"
    ]
  },
  {
    "objectID": "WeekThree.html#principle-component-analysis",
    "href": "WeekThree.html#principle-component-analysis",
    "title": "5  Week Three: Remote Sensing Data and Corrections",
    "section": "5.8 Principle Component Analysis",
    "text": "5.8 Principle Component Analysis\nvariance to remove of variables of dependence , ALso creating group of similar variables such as social variables, environment variables\nand then groups of similar variables\nClip a rectangle in a small study area\n\n5.8.1 Image fusion\n\n\n5.8.2 Pan sharpen\nFootnote: * - Nadir means directly down Empirical Line Correction Topographic Correction Spot m Solar Zenith Angle describing illumination angle of source i.e., sun Digital Number (DN): different radio metric resolution DN is spectral radiance equals radiance calibration TOA is Topographical of Atmosphere reflectance is property of the material whereas radiance is light reflect from source as sun TOA reflectance in the air BOA: property of material DN is the roll number on the camera Hemispherical reflectance: all light enter the satellite or sensor Apperant Reflectance: solar azimuth = compass angle of the sun (N =0°) 90° (E) at sunrise and 270° (W) at sunset. See Azimuth Angle animation solar zenith = angle of local zenith (above the point on ground) and sun from vertical (90° - elevation) Spectral radiance is the amount of light within a band from a sensor in the field of view (FOV) Radiance refers to any radiation leaving the Earth (i.e. upwelling, toward the sensor. It could be also called Top of Atmosphere (TOA) radiance. Irradiance, is used to describe downwelling radiation reaching the Earth from the sun. Reflectance is a property of a material. Digital number (DN): intensity of the electromagnetic radiation per pixel, pixel values that aren’t calibrated and have no unit, have light source, effects of sensor + atmosphere + material, values range from 0 - 255 (Lansat 5) = 8 bit or 0 - 65536 Landsat 8 (12 bit)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week Three: Remote Sensing Data and Corrections</span>"
    ]
  }
]