[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learning Dairy",
    "section": "",
    "text": "Preface\n```",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "WeekOne.html",
    "href": "WeekOne.html",
    "title": "3  Week One: Introduction and Background",
    "section": "",
    "text": "3.1 Summary\nRecent trend on studies around urban area, green space, access to infrastructure, impact on health & well-being has brought the rise of remote sensing cities and environment. NASA defines remote sensing as acquiring information from a distance, by sensors that are mounted on satellites, planes (aerial imagery), drones, phones, free standing on the ground or sea (with hand held devices) etc. (CASA0023). Today, I learnt of basic term used in optics such as earth observation1, satellite2, Sensors3, electromagnetic spectrum4 (figure-1). Then, we got introduced, how does satellite interact with Earth’s surface and few terms were important. Before this radiation are retrieved by the sensors, the electromagnetic radiation (EMR) (e.g. from the sun) experiences surface and atmospheric changes. The surface changes are when energy gets absorbed by the surface energy being transmitted through the surface. There are below term important to understand such as electromagnetic radiation (EMR) 5, radiant energy6, radiant flux7, incoming short-wave radiation or shortwave radiation8, solar irradiance flux10, exitance (emittance) 11 (per unit time - flux).\nFigure-1: Basics of Remote Sensing\n(“Remote Sensing - Components, Types, Working and Applications” 2023)\nWhereas the atmospheric energy can be scattered by particles in the atmosphere. There are three types of atmospheric scattering are Rayleigh12, Mie13 and Non-selective 14. There are important concepts around SAR15 and data format16 that we will learn in detail in coming weeks. Data formats are of 4 types i.e., Spatial Resolution 17, Spectral Resolution 18, Radiometric Resolution 19 and Temporal Resolution 20. Thus, basic components of remote sensing can be seen in figure-2.\nFigure-2: Components of Remote Sensing\n(Abdelhamid, n.d.)\nSensors are designed to monitor bands. Different sensors will have different number of bands 21. Spectral Resolution has the number of bands it records data in, example say spectral resolution 1 22. Thus, there exists multiple spectral bands 23 and hyper spectral bands24. It not always satellite can record the bands from space but from ground too known as spectroradiometer25. Additionally, below (figure-3) are few remote sensing imaging tried during this week.\nFigure-3: Trying Basics of Remote Sensing for Uttarakhand, India",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week One: Introduction and Background</span>"
    ]
  },
  {
    "objectID": "WeekTwo.html",
    "href": "WeekTwo.html",
    "title": "4  Week Two",
    "section": "",
    "text": "4.1 Summary\nIn the first week, this portfolio has content about the basics of remote sensing cities and environment and the presentation learning in Xaringan tool. This week I learnt about the satellites and presentation tools as Xaringan (as attached below) and Quarto. Additionally, we learned how to create book in quarto.\n(Grolemund, n.d.)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week Two</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Abdelhamid, Ghassan. n.d. “BASIC REMOTE SENSING TRAINING OF\nNON-GRADUATE IMAGERY USERS “AN EXPERIENCE FROM UNITED ARAB\nEMIRATES, AIR FORCE GROUND ….”\n\n\nAmani, Meisam, Arsalan Ghorbanian, Seyed Ali Ahmadi, Mohammad Kakooei,\nArmin Moghimi, S. Mohammad Mirmazloumi, Sayyed Hamed Alizadeh Moghaddam,\net al. 2020. “Google Earth Engine Cloud Computing Platform for\nRemote Sensing Big Data Applications: A Comprehensive Review.”\nIEEE Journal of Selected Topics in Applied Earth Observations and\nRemote Sensing 13: 5326–50. https://doi.org/10.1109/JSTARS.2020.3021052.\n\n\nBeker, Teo, Homa Ansari, Sina Montazeri, Qian Song, and Xiao Xiang Zhu.\n2023. “Deep Learning for Subtle Volcanic Deformation Detection\nwith InSAR Data in Central Volcanic Zone.” IEEE Transactions\non Geoscience and Remote Sensing 61: 1–20. https://doi.org/10.1109/TGRS.2023.3318469.\n\n\n“Benefits | Landsat Science.” 2021. https://landsat.gsfc.nasa.gov/benefits/.\n\n\nChen, Wei, Xiaoshen Xie, Jiale Wang, Biswajeet Pradhan, Haoyuan Hong,\nDieu Tien Bui, Zhao Duan, and Jianquan Ma. 2017. “A Comparative\nStudy of Logistic Model Tree, Random Forest, and Classification and\nRegression Tree Models for Spatial Prediction of Landslide\nSusceptibility.” CATENA 151 (April): 147–60. https://doi.org/10.1016/j.catena.2016.11.032.\n\n\nEstoque, Ronald C. 2020. “A Review of the Sustainability Concept\nand the State of SDG Monitoring Using Remote Sensing.” Remote\nSensing 12 (11): 1770. https://doi.org/10.3390/rs12111770.\n\n\nGrolemund, Yihui Xie, J. J. Allaire, Garrett. n.d. Chapter 7\nXaringan Presentations | r Markdown: The Definitive Guide. https://bookdown.org/yihui/rmarkdown/xaringan.html.\n\n\n“Image Classification & Object Based Image Analysis (OBIA) -\nHome - Aerial/Satellite Digital Mapping Solutions - LAND INFO ...\nLandinfo.com.” 2020. https://landinfo.com/image-classification-object-based-image-analysis-obia/.\n\n\nKrzywinski, Martin, and Naomi Altman. 2017. “Classification and\nRegression Trees.” Nature Methods 14 (8): 757–58. https://doi.org/10.1038/nmeth.4370.\n\n\nLaake, Andreas. 2022. “Basics of Remote Sensing.” In,\nedited by Andreas Laake, 3–20. Cham: Springer International Publishing.\nhttps://doi.org/10.1007/978-3-030-73319-3_1.\n\n\nLopez-de-la-Calleja, Miriam, Takayuki Nagai, Muhammad Attamimi, M.\nNakano-Miyatake, and Hector Perez-Meana. 2013. “Object Detection\nUsing SURF and Superpixels.” Journal of Software Engineering\nand Applications 06 (January): 511–18. https://doi.org/10.4236/jsea.2013.69061.\n\n\nMohajane, Meriame, Romulus Costache, Firoozeh Karimi, Quoc Bao Pham, Ali\nEssahlaoui, Hoang Nguyen, Giovanni Laneve, and Fatiha Oudija. 2021.\n“Application of Remote Sensing and Machine Learning Algorithms for\nForest Fire Mapping in a Mediterranean Area.” Ecological\nIndicators 129 (October): 107869. https://doi.org/10.1016/j.ecolind.2021.107869.\n\n\n“National Capital Region Planning Board.” n.d. https://ncrpb.nic.in/policies_strategies.html.\n\n\nPahlevan, Nima, John R. Schott, Bryan A. Franz, Giuseppe Zibordi, Brian\nMarkham, Sean Bailey, Crystal B. Schaaf, Michael Ondrusek, Steven Greb,\nand Christopher M. Strait. 2017. “Landsat 8 Remote Sensing\nReflectance (Rrs) Products: Evaluations, Intercomparisons, and\nEnhancements.” Remote Sensing of Environment 190\n(March): 289–301. https://doi.org/10.1016/j.rse.2016.12.030.\n\n\nParthasarathy, K. S. S., and Paresh Chandra Deka. 2021. “Remote\nSensing and GIS Application in Assessment of Coastal Vulnerability and\nShoreline Changes: A Review.” ISH Journal of Hydraulic\nEngineering 27 (sup1): 588–600. https://doi.org/10.1080/09715010.2019.1603086.\n\n\nPelich, Ramona, Marco Chini, Renaud Hostache, Patrick Matgen, Luca\nPulvirenti, and Nazzareno Pierdicca. 2022. “Mapping Floods in\nUrban Areas from Dual-Polarization InSAR Coherence Data.”\nIEEE Geoscience and Remote Sensing Letters 19: 1–5. https://doi.org/10.1109/LGRS.2021.3110132.\n\n\nPerdikou, Skevi, and Demetris Kouhartsiouk. 2022. “Condition\nAssessment of Levees and Embankments via Satellite Radar\nInterferometry.” In.\n\n\nPerikamana, Krishna Kumar, Krishnachandran Balakrishnan, and Pratyush\nTripathy. 2021. A CNN Based Method for Sub-Pixel Urban Land Cover\nClassification Using Landsat-5 TM and Resourcesat-1 LISS-IV\nImagery.\n\n\nQu, Le’an, Zhenjie Chen, Manchun Li, Junjun Zhi, and Huiming Wang. 2021.\n“Accuracy Improvements to Pixel-Based and Object-Based LULC\nClassification with Auxiliary Datasets from Google Earth Engine.”\nRemote Sensing 13 (3): 453. https://doi.org/10.3390/rs13030453.\n\n\n“Random Forest.” n.d. https://corporatefinanceinstitute.com/resources/data-science/random-forest/.\n\n\n“Remote Sensing - Components, Types, Working and\nApplications.” 2023. https://www.geeksforgeeks.org/remote-sensing/.\n\n\nSaba, Sumbal, Muhammad Ali, Syed Ali Turab, Muhammad Waseem, and Shah\nFaisal. 2022. “Comparison of Pixel, Sub-Pixel and Object-Based\nImage Analysis Techniques for Co-Seismic Landslides Detection in\nSeismically Active Area in Lesser Himalaya, Pakistan.”\nNatural Hazards 114 (October). https://doi.org/10.1007/s11069-022-05642-y.\n\n\nSantosh, K. M., and J. Sundaresan. 2014. “Remote Sensing\nBasics.” In, edited by Janardhanan Sundaresan, K M Santosh,\nAndrea Déri, Rob Roggema, and Ramesh Singh, 279–90. Cham: Springer\nInternational Publishing. https://doi.org/10.1007/978-3-319-01689-4_17.\n\n\nSowmya, K., C. M. John, and N. K. Shrivasthava. 2015. “Urban Flood\nVulnerability Zoning of Cochin City, Southwest Coast of India, Using\nRemote Sensing and GIS.” Natural Hazards 75 (2):\n1271–86. https://doi.org/10.1007/s11069-014-1372-4.\n\n\nTamiminia, Haifa, Bahram Salehi, Masoud Mahdianpari, Lindi Quackenbush,\nSarina Adeli, and B. Brisco. 2020a. “Google Earth Engine for\nGeo-Big Data Applications: A Meta-Analysis and Systematic\nReview.” ISPRS Journal of Photogrammetry and Remote\nSensing, May. https://doi.org/10.1016/j.isprsjprs.2020.04.001.\n\n\n———. 2020b. “Google Earth Engine for Geo-Big Data Applications: A\nMeta-Analysis and Systematic Review.” ISPRS Journal of\nPhotogrammetry and Remote Sensing, May. https://doi.org/10.1016/j.isprsjprs.2020.04.001.\n\n\n“This is the forefront of the SAR image! Viewing images taken by\nICEYE on Tellus!” n.d. https://sorabatake.jp/en/15537/.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "WeekTwo.html#quarto",
    "href": "WeekTwo.html#quarto",
    "title": "4  Week Two",
    "section": "4.2 Quarto",
    "text": "4.2 Quarto\nBookdown was used earlier was compatible with R but not with other programming language. Then came along quarto with different chapters in different markdown. use # for new slide Youtube- share-imbed Index.rmd-don’t change HW: 5-9 slides on sensor put on git/ link in excel shared No code on quarto just text Use kable for making tables reference will be learnt in week 3 use eval = false when a code need not be read to un-render a chapter/ file use ‘_’ underscore in the beginning eg _quarto.yml file in the output dir docs setup a quarto dairy for this week go back to last edit Go to Book, Render, Add, commit, push Do render html and not pdf",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week Two</span>"
    ]
  },
  {
    "objectID": "WeekOne.html#satellite",
    "href": "WeekOne.html#satellite",
    "title": "3  Week One: Introduction and Background",
    "section": "4.1 Satellite",
    "text": "4.1 Satellite\nNASA launched Landsat satellites that changed the method of collecting earth’s landscape data popularly known as remote sensing which interchangeably called as ‘Earth Observation’. Scientist started to get more data and details as the NASA went ahead with launches of Landsat1 to Landsat8. So far to receive enormous detailed data about earth’s resources and climate, the best has been Landsat8, With more sensitive sensors scientist can retrieve improved accuracy in data that helps to manage the earth’s resources and climate optimally. For example, study to most subtle change is vegetation is possible now with Landsat8.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week One: Introduction and Background</span>"
    ]
  },
  {
    "objectID": "WeekOne.html#sensors",
    "href": "WeekOne.html#sensors",
    "title": "3  Week One: Introduction and Background",
    "section": "4.2 Sensors",
    "text": "4.2 Sensors\nSensors are mounted on satellites, drones etc. The sensors monitor electromagnetic rays of spectrum. They are of two types\n\n4.2.1 Passive Sensors\nPassive sensors are those that does not emit energy but uses the energy that is available i.e., from the sun to reflects back energy. The energy reflected back is in electromagnetic waves. For example human eye, camera, satellite sensor.\n\n\n4.2.2 Active Sensors\nThey are those that emit energy. Have an energy source for illumination Actively emits electormagentic waves and then waits to receive Such as: Radar, X-ray, LiDAR Sensors can be mounted on any platform.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week One: Introduction and Background</span>"
    ]
  },
  {
    "objectID": "WeekOne.html#electromagnetic-spectrum",
    "href": "WeekOne.html#electromagnetic-spectrum",
    "title": "3  Week One: Introduction and Background",
    "section": "4.3 Electromagnetic Spectrum",
    "text": "4.3 Electromagnetic Spectrum\nIt is around us all the time, without which the world around us would not exist is the Electromagnetic Radiations. These waves spread across from a very short radiations of gamma rays, x-rays, ultra-violate rays, visible-light waves, infrared waves, micro waves, radio waves. It is collectively known as the electromagnetic Spectrum or EMS. Electromagnetic (EM) waves are energy waves that emits having both electrical and magnetic properties is a two dimensional (2D) property.\nEM waves have crests and troughs like ocean waves and the distance between two crests is known as wavelength, see figure below. While some wavelengths are very long and measured in meters, some are very short and measured in nano-meters. The number of wavelengths passed at a point in 1 second is known as frequency of the wave. One wave or cycle per second is known as a Hertz (Hz).\n\n\n\n\n\n\n\n\n\nSource:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week One: Introduction and Background</span>"
    ]
  },
  {
    "objectID": "WeekOne.html#interacting-with-earths-surface",
    "href": "WeekOne.html#interacting-with-earths-surface",
    "title": "3  Week One: Introduction and Background",
    "section": "4.4 Interacting with Earth’s surface",
    "text": "4.4 Interacting with Earth’s surface\nBefore these radiation are retrieved by the sensors, the electromagnetic radiation(EMR) (e.g. from the sun) experiences surface and atmospheric changes. The surface changes are when energy gets absorbed by the surface energy being transmitted through the surface. There are below term important to understand: - Waves of an electromagnetic field, travel through space and carry radiant energy = Electromagnetic radiation (EMR). Waves are part of the EMR spectrum. - Energy carried by EMR waves = radiant energy -Energy per unit of time = radiant flux - Energy from the sun = incoming short wave radiation or shortwave radiation - Energy (solar power) from the sun per unit area per unit time (from electromagnetic radiation) = solar irradiance (per unit time - flux) - Energy leaving a surface per unit area per unit time = Exitance (emittance) (per unit time - flux) - Flux means time here.\nWhereas the atmospheric energy can be scattered by particles in the atmosphere. There are three types of atmospheric scattering: - Rayleigh = particles are very small compared to the wavelength - Mie = particles are the same size compared to the wavelength - Non selective = particles are much larger than the wavelength.\n\n4.4.1 SAR\nActive sensor such as Synthetic Aperture Radar (SAR) can see through clouds. Bidirectional Reflectance Distribution Function (BRDF) are surface interactions where view (e.g. sensor) and illumination (e.g. sun) angles can change. Energy being reflected from the surface that is smooth or diffuse. Thus, BRDF for flat surface, water surface, vegetation surface have got different texture of surface. SAR can also work on polaristion and fluoresence. Satellite (and aerial) sensors are affected by the atmospheric scattering which can be delt with atmospheric correction. These will be discussed in later weeks. The data cant be directly used as energy reflected from Earth to a sensor. Perhaps, it needs correction due to there many interactions that influence the data being created and we use.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week One: Introduction and Background</span>"
    ]
  },
  {
    "objectID": "WeekOne.html#data-format",
    "href": "WeekOne.html#data-format",
    "title": "3  Week One: Introduction and Background",
    "section": "4.1 Data Format",
    "text": "4.1 Data Format\nIn the majority of cases remotely sensed data is raster. Data format is generally geo.tif. Raster data are stacked vertically in B!, B2, B3 etc.(insert img). LiDAR data points has x,y with the z-dimension to collect attitude and 3D analysis. Data format are of following 4 types:\n\nSpatial Resolution\n\nSpatial resolution has the size of the raster grid per pixel (e.g. 20cm or 30m). One can compare the spatial resolution of 10 by 10 cm vs 1 by 1 km.\n\nSpectral Resolution\n\nSpectral Signature is unique signature given to material on earth or its combination shown through a graph . Sensors are designed to monitor specific range of EM spectrum called band. Different sensors will have different number of bands. Spectral Resolution has the number of bands it records data in. Spectral resolution 1 means Bands: range of spectrum, where more rich the band, will have more elements to identify. Spectral resolution means to observe part of spectrum in the window.\nThe 7-12 bands are known as multiple spectral bands where as 10-25 bands are hyper spectral bands which has continuous spectrum. It not always satellite can record the bands but ground too known as spectroradiometer which are pure white refernce panels that needs to be calibrated.\n\nRadiometric Resolution It identifies differences in light or reflectance, in practice this is the range of possible values. The ability of a sensor to identify and show small differences in energy. The higher the resolution, the more sensitive it is i.e., resolution of 8 bit can have 256 possible values where a resolution of 4 bit has 16 possible values. The more sensitive, the more components to show where as lower the radiometric resolution the lower the quality of the image and possibility to differentiate features.\nTemporal Resolution\n\nThis is the resolution means the time it revisits (e.g. daily, every 7 days, on demand). Its low resolution means it has a large pixel size (e.g. MODIS is 500m by 500m pixel).\n\n4.1.1 Other Considerations\ngeosynchronous orbit (GSO) if the satellite matches the Earth’s rotation or if geostationary orbit holds same position, usually only for communications but some sensors are geostationary.\nIn order to carry data collection for a research question the answers should be looked is about will dictate what sensor is most appropriate size of features, date range, revisit requirement, spectral sensitivity, cost.\nHeads-up for practical: Loading Landsat and Sentinel data, SNAP and R, regions of interest and plot.\n\n\n4.1.2 Electromagnetic Spectrum\nLandsat vs Sentinel B1, B2, B3 B4 (insert img) and graphs (inster img)\n\n\n4.1.3 Atmosphere Correction\n\n\n4.1.4 SNAP\nonly 3 bands Spectral feature space (insert img of scatter plot) then we do a tassel cap transformation to combine coefficient to simplify. Principle Component Analysis (PCA) is carried to simplify on dryness for urban, brightness for greeness, wetness for yellow. (insert img). Although there are various way to carry PCA that are readily available e.g., Normalized Difference in Vegetation Index (NDVI).\nNDVI = insert img there are also healthy green vegetation\nNDWI: Normalized Difference in Water Index NDSI: Normalized Difference in Soil Index NDBI: Normalized Difference in Built-up Index\nthe study area could be spread over two tiles (insert img) and different dates. Thus, we combine and take step by step process.\nHowever, in Google Earth Engine (GEE) take all the titles of different dates and calculates the median to calculate …..\nSNAP when open images\nhigh albido:\n10m or landsat 30 m (different spect./ scale) upscaling or downscaling resampling Machine Learning (ML) is used to learn land cover change Spectral Signature vs Spectral Libraries\nRegion of Interest:\n\nPIF: suedo invariant features\n\nw\n\n\ndownload\n\n\nunderstand bands end member analysis/ MESMA\n\n\nsatellite Segments",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week One: Introduction and Background</span>"
    ]
  },
  {
    "objectID": "About.html",
    "href": "About.html",
    "title": "2  About the Learner, the Module and the Porfolio",
    "section": "",
    "text": "2.1 The Learner\nI am Geetanjli Rani, a keen learner because it liberates me to evolve. With the same zeal, I wish to learn the module of CASA0023 on the Remote Sensing Cities and Environment. Let us see my journey of evolution to grasp the outcome and objectives of the opted module. All the best to ME!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>About the Learner, the Module and the Porfolio</span>"
    ]
  },
  {
    "objectID": "About.html#the-module",
    "href": "About.html#the-module",
    "title": "2  About the Learner, the Module and the Porfolio",
    "section": "2.2 The Module",
    "text": "2.2 The Module\nThis the module of CASA0023 about the Remote Sensing Cities and Environment. The lecture happens for and hour and the next hour is for drop in session followed by practical scheduled on the other day. Assignment would comprise of two parts. First group submission to present Xaringan power point in week 10 that holds 30 percent of weightage. The second submission of 70 percentage weightage is about the learning dairy in Quarto which includes week wise notes into sections/ 4 paragraphs of summary, application and reflection with references. The learning dairy is an informal notes writing with diagrams and mind-mapping with a word limit of 4000 and 2 pages a week. Few of the best submissions samples are available from last year on the slack channel CASA0023. In the interest of the module, the Remote Sensing of the Environment, An Earth Resource Perspective by Jensen and Google Earth Engine handbook are the two important notebooks. The aims of this module will enable to operationalise remotely sensed Earth observation data for informing decisions on environmental hazards arising from a changing climate, specifically in relation to (a) urban areas and (b) future urban sustainability. Additionally, one could use the remotely sensed (RS) data and analysis to bring policy about managing earth’s resources and address climate change.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>About the Learner, the Module and the Porfolio</span>"
    ]
  },
  {
    "objectID": "About.html#the-portfolio",
    "href": "About.html#the-portfolio",
    "title": "2  About the Learner, the Module and the Porfolio",
    "section": "2.3 The Portfolio",
    "text": "2.3 The Portfolio\nIn order to understand my portfolio, one needs to get in my head. Thus, I will take you through a series of cool abbreviations, dictionary/glossary and figures introduced to me by our very intuitive Prof Andrew Maclachlan whom we call Prof Andy. Although, some the glossary/terms looked new for me while most of them were known, I am re-brushing my knowledge and kind of putting the jigsaw-puzzle-pieces together towards self challenge to the quest of learning the Remote Sensing Cities and Environment. These method of data collecting by remotely sensed devices are very fascinating to me that I have made an attempt to design this portfolio as personal journal of experience learning CASA0023. Thus, one may see the sections of summary, application and reflection. When arrive at the reflection part, one may witness my personnel experience in terms of my thoughts, ideas, struggles etc. Well! I am having fun designing my portfolio, I hope you feel have the same flipping it over!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>About the Learner, the Module and the Porfolio</span>"
    ]
  },
  {
    "objectID": "About.html#learning-outcomes",
    "href": "About.html#learning-outcomes",
    "title": "2  About the Learner, the Module and the Porfolio",
    "section": "2.4 Learning Outcomes",
    "text": "2.4 Learning Outcomes\nOld Terms:  New Terms: Spectral Signature",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>About the Learner, the Module and the Porfolio</span>"
    ]
  },
  {
    "objectID": "WeekThree.html",
    "href": "WeekThree.html",
    "title": "5  Week Three: Remote Sensing Data: Corrections and Imaging",
    "section": "",
    "text": "5.1 Summary\nBefore going ahead to understand the image correction and processing, it is worth knowing few interesting things about the remote sensing or earth observation.\nHowever, there were new information came to light for me as can be seen below.\n(“Benefits | Landsat Science” 2021)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week Three: Remote Sensing Data: Corrections and Imaging</span>"
    ]
  },
  {
    "objectID": "WeekThree.html#corrections",
    "href": "WeekThree.html#corrections",
    "title": "5  Week Three: Remote Sensing Data and Corrections",
    "section": "",
    "text": "5.1.1 Geometric Correction\nDepending upon the given coordinate reference system, the remotely sensed data has collected image distortions can be introduced due to the view angle (off-nadir*), topography (e.g. hills not flat ground), wind (if from a plane), rotation of the earth (from satellite).\nTp deal with geometric distortions, Ground Control Points (GPS) are identified to match known points in the image and a reference dataset such as a local map, another image, GPS data from handheld device etc. The coordinates are modeled to give geometric transformation coefficients through the method of linear regression with the distorted x or y as the dependent or independent*. This is the plotted to minimise the RMSE where Jensen sets a RMSE value of 0.5. A slight shift in the data is the required. So, re-sampling the final raster by method of nearest neighbor, linear, cubic, cubic spline is done. Thus, the input grid to output grid to re-sampling.\n\n\n5.1.2 Atmospheric Correction\nIdeally there are two most important sources of environmental attenuation, the atmospheric scattering and topographic attenuation. Types of particle scattering (insert image) for Relative (to something) is done in two ways, the DOS and PIF. The Dark object subtraction (DOS) or histogram adjustment, searches each band for the darkest value then subtracts that from each pixel. Where as psuedo-invariant features (PIFs), assumes brightness pixels linearly related to a base image and applies regression per band, Then adjust the image based on the regression result. It needs to be noticed that y is the value of the base. So, to get y we multiply our new date pixel (x) by the coefficient and add the intercept value. Similarly, it is applies to the rest of the pixels.\nAbsolute (definitive) Correction requires to change digital brightness values into scaled surface reflectance. Thereafter, compare these scaled surface reflectance values across the planet. It can be done through atmospheric radiative transfer models from various options. The atmopshierc radiative transfer code such as MODTRAN 4+ and the Second Simulation of the Satellite Signal in the Solar Spectrum (6S) gives the scattering and absorption information which can now be used through python - called Py6S.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week Three: Remote Sensing Data and Corrections</span>"
    ]
  },
  {
    "objectID": "WeekThree.html#emprical-line-correction",
    "href": "WeekThree.html#emprical-line-correction",
    "title": "5  Week Three: Remote Sensing Data and Corrections",
    "section": "5.2 Emprical Line Correction",
    "text": "5.2 Emprical Line Correction\nThe in-situ measurements taken using a field spectrometer that requires measurements at the same time as the satellite overpass. Next, applying the linear regression to the measurements against the satellite data raw digital number in the following equation:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week Three: Remote Sensing Data and Corrections</span>"
    ]
  },
  {
    "objectID": "WeekThree.html#orthorectification-correction-topographic-correction",
    "href": "WeekThree.html#orthorectification-correction-topographic-correction",
    "title": "5  Week Three: Remote Sensing Data and Corrections",
    "section": "5.3 Orthorectification correction / topographic correction",
    "text": "5.3 Orthorectification correction / topographic correction\nThe view taken at an angle can create such image distortion where georectification is giving coordinates to an image and orthorectification is removing distortions. Thus, making the pixels viewed at nadir is a straight down view. It requires sensor geometry and an elevation model and corrected through the Software / formulas of Jensen as below:\nVarious softwares such as QGIS, SAGA GIS, R package topocorr, R package RStoolbox etc. for topographic corrections where atmospheric corrections needs to be done before this.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week Three: Remote Sensing Data and Corrections</span>"
    ]
  },
  {
    "objectID": "WeekThree.html#radiometric-calibration",
    "href": "WeekThree.html#radiometric-calibration",
    "title": "5  Week Three: Remote Sensing Data and Corrections",
    "section": "5.4 Radiometric Calibration",
    "text": "5.4 Radiometric Calibration\nPre-calibration (below formula) is required in a lab before a sensor is launched, it is then uses these measurements to adjust the data from the sensor. The image brightness and distribution as a Digital Number (or DN) gets captured by the sensors, converted to spectral radiance is the radiometric calibration (in below formula).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week Three: Remote Sensing Data and Corrections</span>"
    ]
  },
  {
    "objectID": "WeekThree.html#landsat-ard-surface-reflectance",
    "href": "WeekThree.html#landsat-ard-surface-reflectance",
    "title": "5  Week Three: Remote Sensing Data and Corrections",
    "section": "5.5 Landsat ARD & surface reflectance",
    "text": "5.5 Landsat ARD & surface reflectance\nLandsat ARD: already validated level 2 product that provides corrected images for data processing. The Landsat Ecosystem Disturbance Adaptive Processing System (LEDPAS) and the Landsat 8 Surface Reflectance algorithm (L8SR) could make this possible. Noted be about the Level 2 product means something has changed as may come across not a ARD, e.g. very high resolution, drone.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week Three: Remote Sensing Data and Corrections</span>"
    ]
  },
  {
    "objectID": "WeekThree.html#joining-data-sets",
    "href": "WeekThree.html#joining-data-sets",
    "title": "5  Week Three: Remote Sensing Data and Corrections",
    "section": "5.6 Joining data sets",
    "text": "5.6 Joining data sets\nTo creates a seamless mosaic or image(s) the feather of images together is carried in Remote Sensing. The seamline is dividing line with a base image and “other” or second image with 20-30 percent overlap a histogram is extracted. Next, a histogram matching algorithm is performed which gives similar brightness values of the two images and feathering is conducted.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week Three: Remote Sensing Data and Corrections</span>"
    ]
  },
  {
    "objectID": "WeekThree.html#enhancements",
    "href": "WeekThree.html#enhancements",
    "title": "5  Week Three: Remote Sensing Data and Corrections",
    "section": "5.7 Enhancements",
    "text": "5.7 Enhancements\nImagery can be “improved/enhanced” based on the energy reflected and the contrast between features But… How do these methods help in urban environments Does adding complexity to imagery (or creating new datasets) assist us in our aim?\nIt is contrasting images Other enhancements: NDVI, NBR Local enhancements: Edge enhancements: embossing, filter for example\nOriginal - low pass - High pass",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week Three: Remote Sensing Data and Corrections</span>"
    ]
  },
  {
    "objectID": "WeekThree.html#principle-component-analysis",
    "href": "WeekThree.html#principle-component-analysis",
    "title": "5  Week Three: Remote Sensing Data and Corrections",
    "section": "5.8 Principle Component Analysis",
    "text": "5.8 Principle Component Analysis\nvariance to remove of variables of dependence , ALso creating group of similar variables such as social variables, environment variables\nand then groups of similar variables\nClip a rectangle in a small study area\n\n5.8.1 Image fusion\n\n\n5.8.2 Pan sharpen\nFootnote: * - Nadir means directly down Empirical Line Correction Topographic Correction Spot m Solar Zenith Angle describing illumination angle of source i.e., sun Digital Number (DN): different radio metric resolution DN is spectral radiance equals radiance calibration TOA is Topographical of Atmosphere reflectance is property of the material whereas radiance is light reflect from source as sun TOA reflectance in the air BOA: property of material DN is the roll number on the camera Hemispherical reflectance: all light enter the satellite or sensor Apperant Reflectance: solar azimuth = compass angle of the sun (N =0°) 90° (E) at sunrise and 270° (W) at sunset. See Azimuth Angle animation solar zenith = angle of local zenith (above the point on ground) and sun from vertical (90° - elevation) Spectral radiance is the amount of light within a band from a sensor in the field of view (FOV) Radiance refers to any radiation leaving the Earth (i.e. upwelling, toward the sensor. It could be also called Top of Atmosphere (TOA) radiance. Irradiance, is used to describe downwelling radiation reaching the Earth from the sun. Reflectance is a property of a material. Digital number (DN): intensity of the electromagnetic radiation per pixel, pixel values that aren’t calibrated and have no unit, have light source, effects of sensor + atmosphere + material, values range from 0 - 255 (Lansat 5) = 8 bit or 0 - 65536 Landsat 8 (12 bit)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week Three: Remote Sensing Data and Corrections</span>"
    ]
  },
  {
    "objectID": "WeekThree.html#summary",
    "href": "WeekThree.html#summary",
    "title": "5  Week Three: Remote Sensing Data: Corrections and Imaging",
    "section": "",
    "text": "5.1.1 Correction\nThe entire process has been explained in figure-1. Corrections are required because satellite imagery can contain errors from various sources, including the sensor, atmosphere, and terrain. Scan line correction (SLC) is applied to align images from whisk broom, spotlight, or across-track scanners. This alignment is achieved through regression, where the vertical differences between the blue line and residuals should sum to 0. Therefore, these errors must be corrected as necessary to ensure accuracy. Contextualizing the use of imagery is essential for this purpose. Additionally, linear regression is commonly employed for modeling and then resampling the data.\nA Geometric Correction is necessary to address image distortions caused by factors such as the coordinate reference system, view angle, topography, wind, and rotation of the Earth. This correction involves identifying Ground Control Points (GPS) to match known points in the image with reference data, such as a local map, another image, or GPS data from a handheld device.\nThe second correction, Atmospheric Correction (AC), is essential to mitigate environmental attenuation, including atmospheric scattering and topographic effects. Relative correction methods, such as Dark Object Subtraction (DOS) or histogram adjustment, involve identifying the darkest value in each band and subtracting it from each pixel. Pseudo-Invariant Features (PIFs) are also used for relative correction. Absolute Correction entails converting digital brightness values into scaled surface reflectance. Empirical Line Correction involves in-situ measurements using a field spectrometer, conducted simultaneously with satellite overpasses. Linear regression is then applied to correlate these measurements with the raw digital number data from the satellite.\nThe third correction, Orthorectification / Topographic Correction, is essential because imagery taken at an angle can cause distortions. Georectification assigns coordinates to an image, while orthorectification removes these distortions.\nLastly, Radiometric Calibration involves pre-calibrating sensors in a lab before launch and then using these measurements to adjust sensor data. This process involves capturing image brightness and distribution as a Digital Number (DN) by the sensors, which is then converted to spectral radiance through radiometric calibration.\n\n\n5.1.2 Joining data sets\nTo create a seamless mosaic or image(s) in Remote Sensing, the process of feathering images together is utilized. The seamline serves as the dividing line between a base image and the “other” or second image, with a recommended 20-30 percent overlap, from which a histogram is extracted. Subsequently, a histogram matching algorithm is applied to achieve similar brightness values between the two images, followed by the feathering process.\nEnhancements of imagery can be achieved based on the energy reflected and the contrast between features. Various enhancements include NDVI, NBR, and local enhancements such as edge enhancements using techniques like embossing or filtering. Principle Component Analysis (PCA) serves as a method to remove variance from dependent variables and is also utilized to group similar variables, such as social or environmental variables. A PCA (figure-2) and texture (figure-3) analysis has been show for Delhi, India as carried during the practilce CASA0023.\nFigure-1: Remote Sensing Process\n\n\n\n\n\n\n\n\n\nFigure-2: NDVI Enhancement for Delhi, India\n\n\n\n\n\n\n\n\n\nFigure-3: PCA for Delhi, India\n\n\n\n\n\n\n\n\n\nFigure-4: Texture Analysis for Delhi India",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week Three: Remote Sensing Data: Corrections and Imaging</span>"
    ]
  },
  {
    "objectID": "WeekThree.html#application",
    "href": "WeekThree.html#application",
    "title": "5  Week Three: Remote Sensing Data: Corrections and Imaging",
    "section": "5.2 Application",
    "text": "5.2 Application\nIn T. Therry’s discussion on three-dimensional (3D) geometric processing and ortho-rectification of remote sensing (RS) data, the author emphasizes the importance of this process as the first step in multi-source multi-format data fusion within geographic information systems. The fusion of image data (such as visible and microwave, panchromatic and multi-bands, polarimetric bands, passive and active, etc.), along with their metadata (GPS, star trackers, inertial systems, lens and focal plane data, etc.), and associated 3D cartographic data (ground control points, contour lines, digital terrain models (DEM), planimetric features, etc.), is essential for performing precise 3D geometric correction and subsequent ortho-rectification processes using DEM. The article provides insights into the state-of-the-art of geometric correction, including sources of geometric distortions, mathematical models, methods, algorithms, and processing steps, while also addressing error propagation during the fusion of RS and cartographic data from image acquisition to ortho-rectification processes.\nIn another article by (Pahlevan et al. 2017), the application of spectroradiometer is discussed, particularly focusing on the Operational Land Imager (OLI) onboard Landsat-8. The OLI provides high-quality aquatic science products, with remote sensing reflectance (Rrs) being a critical product, defined as the ratio of water-leaving radiance to total downwelling irradiance just above water. The authors extensively assess the quality of Rrs products derived from OLI imagery under near-ideal atmospheric conditions in moderately turbid waters. They note slight across-track non-uniformities (&lt;1%) associated with OLI scenes in the blue bands, and observe that OLI products generally exhibit larger radiometric responses in the blue channels compared to Aqua platform (MODISA) products. After implementing updated vicarious calibration gains and accounting for across-track non-uniformities, matchup analyses using independent in-situ validation data confirm improvements in Rrs products. These findings support the high-fidelity of OLI-derived aquatic science products, demonstrating robust atmospheric correction methods and consistent product quality across OLI’s imaging swath.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week Three: Remote Sensing Data: Corrections and Imaging</span>"
    ]
  },
  {
    "objectID": "WeekThree.html#reflection",
    "href": "WeekThree.html#reflection",
    "title": "5  Week Three: Remote Sensing Data: Corrections and Imaging",
    "section": "5.3 Reflection",
    "text": "5.3 Reflection\nIn my previous experience, I utilized Google Earth images for spatial analysis, encountering challenges with angle correction, image distortion, and scaling. Overcoming these obstacles required a laborious process, including looping through corrections and downloading map sections for improved resolution to facilitate local-level spatial analysis, particularly for stitching basemaps. Fortunately, the recent lecture has significantly enhanced my understanding and proficiency in remote sensing (RS) and Earth observation (EO).\nI now possess knowledge of various image correction and enhancement techniques. Moreover, I have gained insights into addressing challenges like cloudy conditions by leveraging SAR data, which I realize can also be applicable to mitigating issues during fog, smog, and smoke special of case in India. The concept of tiling resonates with me, and I found the Dark Object Subtraction (DOS) method for atmospheric correction particularly enlightening. Understanding subtle distinctions between key concepts such as radiance (or DN) versus reflectance has piqued my interest. Additionally, the practical application of various normalization indexes, such as NDVI, NDBI etc. which I encountered frequently in previous RS research, now holds greater significance for me.\nFootnote: * - Nadir means directly down Empirical Line Correction Topographic Correction Spot m Solar Zenith Angle describing illumination angle of source i.e., sun Digital Number (DN): different radio metric resolution DN is spectral radiance equals radiance calibration TOA is Topographical of Atmosphere reflectance is property of the material whereas radiance is light reflect from source as sun TOA reflectance in the air BOA: property of material DN is the roll number on the camera Hemispherical reflectance: all light enter the satellite or sensor Apperant Reflectance: solar azimuth = compass angle of the sun (N =0°) 90° (E) at sunrise and 270° (W) at sunset. See Azimuth Angle animation solar zenith = angle of local zenith (above the point on ground) and sun from vertical (90° - elevation) Spectral radiance is the amount of light within a band from a sensor in the field of view (FOV) Radiance refers to any radiation leaving the Earth (i.e. upwelling, toward the sensor. It could be also called Top of Atmosphere (TOA) radiance. Irradiance, is used to describe downwelling radiation reaching the Earth from the sun. Reflectance is a property of a material. Digital number (DN): intensity of the electromagnetic radiation per pixel, pixel values that aren’t calibrated and have no unit, have light source, effects of sensor + atmosphere + material, values range from 0 - 255 (Lansat 5) = 8 bit or 0 - 65536 Landsat 8 (12 bit)\n\n\n\n\n“Benefits | Landsat Science.” 2021. https://landsat.gsfc.nasa.gov/benefits/.\n\n\nPahlevan, Nima, John R. Schott, Bryan A. Franz, Giuseppe Zibordi, Brian Markham, Sean Bailey, Crystal B. Schaaf, Michael Ondrusek, Steven Greb, and Christopher M. Strait. 2017. “Landsat 8 Remote Sensing Reflectance (Rrs) Products: Evaluations, Intercomparisons, and Enhancements.” Remote Sensing of Environment 190 (March): 289–301. https://doi.org/10.1016/j.rse.2016.12.030.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week Three: Remote Sensing Data: Corrections and Imaging</span>"
    ]
  },
  {
    "objectID": "WeekFour.html",
    "href": "WeekFour.html",
    "title": "6  WeekFour: Policy and Remote Sensing",
    "section": "",
    "text": "6.1 Summary",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>WeekFour: Policy and Remote Sensing</span>"
    ]
  },
  {
    "objectID": "WeekFour.html#summary",
    "href": "WeekFour.html#summary",
    "title": "6  WeekFour: Policy and Remote Sensing",
    "section": "",
    "text": "6.1.1 Policy Application and Earth Observation Data\nThere exists a wide variety of EO data available for use. These data come from remote sensing sensors and depend on factors such as spectral bands, resolutions, and cost. Examples include multi-temporal land cover mapping, spectral signatures or libraries, change detection for urban or forest areas, vegetation stress detection for illegal logging, precipitation measurement, elevation models (such as LiDAR), temperature monitoring, night-time lights for urban development analysis, forest fire monitoring and prediction, pollution monitoring, drought indices, informal housing detection, water level monitoring, extraction of building or network outlines (polygons/lines), environmental monitoring (e.g., Aral Sea), and estimations of resources like forests, water bodies, snow, ice, and green space.\n\n\n6.1.2 Challenges in the State-of-Art-of-Policy-Making\nThe global policy documents outline the New Urban Agenda, focusing on standards and principles for urban planning, development, and management. Subsections 64, 65, and 67 address concerns specific to coastal areas, delta regions, and small island developing states. Sustainable Development Goals (SDGs), particularly Goal 11, aim to make cities inclusive, safe, resilient, and sustainable, with target 11.5 emphasizing data collection frameworks for disaster studies. RS and EO data are incorporated into targets 11.6 and 11.7, but their complex approach raises questions about target communities, replication, and usefulness of results. Metropolitan policy documents from cities like London, NYC, Cape Town, and Ahmadabad lack data specifications and applicability, while local planning authorities often focus on legislation and monitoring rather than prevention.\nFigure-1: Sustainable Development Goals using Remote Sensing\n\n\n\n\n\n\n\n\n\n(Estoque 2020)\nRemote sensing offers a systematic, data-driven approach to decision-making. For instance, in the figure-2 of Delhi-National Capital Region (NCR), which hosts over 71 million people, policies aim to manage physical growth and development pressures. Participating states are tasked with preparing sub-plans focusing on transportation, civic infrastructure, land use, and conservation. The NCR is divided into various policy zones, including NCT-Delhi, DMA, Rest of NCR, Highway Corridor Zone (HRZ), and Natural Conservation Zone (NCZ), to achieve these objectives.\nFigure-2: National Capital Region (NCR) Delhi, Policy Zones\n\n\n\n\n\n\n\n\n\n(“National Capital Region Planning Board,” n.d.)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>WeekFour: Policy and Remote Sensing</span>"
    ]
  },
  {
    "objectID": "WeekFour.html#application",
    "href": "WeekFour.html#application",
    "title": "6  WeekFour: Policy and Remote Sensing",
    "section": "6.2 Application",
    "text": "6.2 Application\nThe authors (Parthasarathy and Deka 2021) K.S.S. Parthasarathy and Paresh Chandra Deka review the remote sensing and GIS applications in assessing coastal vulnerability and shoreline changes. They examine shoreline changes and the Coastal Vulnerability Index (CVI), offering valuable insights for the literature to identify critical parameters that significantly impact the coast in specific studies. However, the review article overlooks the application of remote sensing assistance to regional bodies in mitigating coastal vulnerability and shoreline changes. Interestingly, (Sowmya, John, and Shrivasthava 2015) K. Sowmya et al. conducted a study on urban flood vulnerability zoning in Cochin City, situated on the southwest coast of India, using remote sensing in conjunction with GIS tools. They assessed vulnerability by employing a multi-criteria evaluation approach in a GIS environment, utilizing remotely sensed images, SRTM DEM, census details, city maps, and field studies. The researchers delineated flood vulnerability zones into categories ranging from low to very high vulnerability. They also calculated the proportion of area classified as very high and high vulnerability and examined the nature of vulnerability. However, their study leaves readers questioning how to implement remote sensing and GIS-driven analyses at the spatial level.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>WeekFour: Policy and Remote Sensing</span>"
    ]
  },
  {
    "objectID": "WeekFour.html#reflection",
    "href": "WeekFour.html#reflection",
    "title": "6  WeekFour: Policy and Remote Sensing",
    "section": "6.3 Reflection",
    "text": "6.3 Reflection\nI liked the aspect where we continue to build on the knowledge gained from the previous module, CASA0005-GIS, and apply techniques like linear regression, spatial auto-regression (SAR), lag analysis, etc. It’s evident that standalone remote sensing (RS) tools are insufficient and require integration with GIS tools. There seems to be a fragmented effort in terms of tools and techniques to address spatial problems, as reflected in the referenced articles in the application section. The discussion on different sensor data was particularly insightful, and the addition of InSAR learning to last week’s SAR knowledge was beneficial.\nFurthermore, the utilization of Earth Observation (EO) data seems to be overlooked in these efforts. Therefore, informed EO data policies and decision-making processes can help achieve goals and improve urban areas more effectively in a data-informed manner. To facilitate such learning, the professor designed a group exercise aimed at presenting a case study to the relevant government authorities. The objective is to propose one or more suitable remote sensing applications, along with pertinent data and details, to address pressing issues at the metropolitan level.\n\n\n\n\nEstoque, Ronald C. 2020. “A Review of the Sustainability Concept and the State of SDG Monitoring Using Remote Sensing.” Remote Sensing 12 (11): 1770. https://doi.org/10.3390/rs12111770.\n\n\n“National Capital Region Planning Board.” n.d. https://ncrpb.nic.in/policies_strategies.html.\n\n\nParthasarathy, K. S. S., and Paresh Chandra Deka. 2021. “Remote Sensing and GIS Application in Assessment of Coastal Vulnerability and Shoreline Changes: A Review.” ISH Journal of Hydraulic Engineering 27 (sup1): 588–600. https://doi.org/10.1080/09715010.2019.1603086.\n\n\nSowmya, K., C. M. John, and N. K. Shrivasthava. 2015. “Urban Flood Vulnerability Zoning of Cochin City, Southwest Coast of India, Using Remote Sensing and GIS.” Natural Hazards 75 (2): 1271–86. https://doi.org/10.1007/s11069-014-1372-4.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>WeekFour: Policy and Remote Sensing</span>"
    ]
  },
  {
    "objectID": "WeekFive.html",
    "href": "WeekFive.html",
    "title": "7  WeekFive: Google Earth Engine (GEE)",
    "section": "",
    "text": "7.1 Summary",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>WeekFive: Google Earth Engine (GEE)</span>"
    ]
  },
  {
    "objectID": "WeekSix.html",
    "href": "WeekSix.html",
    "title": "8  WeekSix: Classification",
    "section": "",
    "text": "8.1 Summary\nRecapping from last week, Google Earth Engine (GEE) offers rapid handling of vast volumes of data, providing a systematic and computer-controlled environment for processing, classification, accuracy improvement, etc., for pixel imagery data. In remote sensing (RS) cloud computation, the classification of pixel imagery data occurs in two main ways: supervised and unsupervised. Supervised classification requires significant control and input details regarding the classification of pixel data, relying on outputs from pattern recognition or machine learning. The classifier learns patterns in the data and uses them to assign labels to new data. This pattern vector is then used by analysts to classify the image. On the other hand, unsupervised classification involves minimal interference, particularly in cases where land cover classes are not known beforehand. It requests the computer to cluster the data based on information such as bands and then label the clusters accordingly. Machine learning-based classification, which focuses on pattern recognition for thematic information extraction, is also referred to as intuitive learning.\nIn remote sensing, digital image processing also employs information extraction using artificial intelligence (AI) techniques such as expert systems, support vector machines, and random forest classifiers. Finally, the error matrix is used to derive thematic map accuracy assessments using GEE in RS. As urban spatial science professionals, it is crucial to understand the underlying requirements of geospatial cloud computation, enabling us to be aware and critical of its processes and applications in future endeavors.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>WeekSix: Classification</span>"
    ]
  },
  {
    "objectID": "WeekSeven.html",
    "href": "WeekSeven.html",
    "title": "9  WeekSeven: Classification-2 The Big Question",
    "section": "",
    "text": "9.1 Summary\nData processing is essential in handling EO data. When processing EO data, it’s crucial to preprocess the data to correct for atmospheric effects, geometric distortions, and radiometric variations. This ensures that the data is suitable for analysis. Processing EO data involves various considerations, including data pre-processing, choosing between pixel and object-based analysis, addressing mixed pixels and objects, and assessing model accuracy using independent reference data and appropriate evaluation metrics. Additionally, accounting for spatial autocorrelation and using advanced techniques like OBIA and spatial cross-validation can enhance the accuracy of classification models.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>WeekSeven: Classification-2 The Big Question</span>"
    ]
  },
  {
    "objectID": "WeekEight.html",
    "href": "WeekEight.html",
    "title": "10  WeekEight: Temperature",
    "section": "",
    "text": "10.1 Summary",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>WeekEight: Temperature</span>"
    ]
  },
  {
    "objectID": "WeekFive.html#summary",
    "href": "WeekFive.html#summary",
    "title": "7  WeekFive: Google Earth Engine (GEE)",
    "section": "",
    "text": "7.1.1 Google Earth Engine: Setup\nWe delved into the utilization of vector data in Earth Observation (EO) during the pre-reading week, where we familiarized ourselves with tools such as SNAP and QGIS. My understanding took a revolutionary leap with the introduction of the Google Earth Engine tool. Earth Engine, developed by Google, is capable of performing massive-scale computations and is often referred to as a planetary computer. In Earth Engine, images are inherited from raster data, with each image comprising one or more bands, each characterized by its own name, data type, scale, mask, and projection. On the other hand, features represent vector data, consisting of geometry with associated attributes. Moreover, a collection of several images or polygons is referred to as an image stack, while a collection of features is termed a feature stack, encompassing multiple polygons. The potential for enormous processing in Earth Engine is supported by JavaScript, which enables the execution of multiple parallel computations across CPUs, including those on Google servers, a process commonly known as cloud computing. Furthermore, Earth Engine employs Earth Engine Objects, denoted by names prefixed with ‘ee,’ with code executed on the server side rather than the client side (i.e., the browser). Lastly, Earth Engine offers the advantage of mapping over looping; while looping entails iterative processing that can be cumbersome, mapping in Earth Engine distributes processing across various machines, enhancing efficiency.\n\n\n7.1.2 GEE functions and tools\nThe second part of the lecture covered the functions and tools utilized in Google Earth Engine (GEE). Objects such as vector, raster, feature, string, and number constitute the foundational elements of GEE. Each of these belongs to a class and is accessed through specific functions, methods, or constructs within GEE. Geometric operations, including spatial join, filtering, and zonal statistics, are integral components of typical GEE operations. Furthermore, GEE supports methods of machine learning, such as supervised and unsupervised classification, deep learning, and exploring relationships between variables, enabling the generation of applications and outputs in the form of online charts, scalable geospatial applications, and query data with user interfaces.\nIn GEE, object-oriented learning is facilitated through the use of reducer objects to compute statistics or perform aggregations. Reducers can aggregate data over time, space, bands, lists, and other data structures within Earth Engine. Each reducer can accept one or more inputs and generate one or more outputs. Image reduction can be performed by region, regions, or neighborhoods (utilizing kernels). GEE offers the advantage of accessing imagery from multiple sensors, allowing for the examination of changes over time in pixel values using functions such as linearFit() with the reducer function. This process, which can include multiple dependent variables, is termed multivariate multiple linear regression.\nSimilarly, join and filter operations can be applied within GEE. In the practice session, a super quick Principal Component Analysis (PCA) and texture analysis were conducted for Delhi, as illustrated in Figure 1, which significantly reduced the processing time compared to the previous week.\nFigure-1: PCA and Texture analysis in GEE",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>WeekFive: Google Earth Engine (GEE)</span>"
    ]
  },
  {
    "objectID": "WeekFive.html#literature-findings",
    "href": "WeekFive.html#literature-findings",
    "title": "8  WeekFive: Google Earth Engine (GEE)",
    "section": "8.2 Literature Findings",
    "text": "8.2 Literature Findings\nWhen reviewed in recent research articles, it is evident both the study by Tamiminia, H. et al. and Amani, M. et al. inferred that the use of GEE is still evolving in the field of remote sensing ever since its launch in 2010. However, Tamiminia, H. et al. have brought a summary of algorithms and capabilities in term of packages that are available, such as machine learning, image processing, image collection, geometry-feature, reducer, charts and specialized algorithms to explore the setup of GEE application in RS (refer figure-1). On the other hand, the team of Amani, M. et al. describes the application of GEE that was used along with version 1 Tropical Rainfall Measuring Mission (TRMM) precipitation products to study the spatial and temporal patterns of precipitation in the Zambezi River basin. They further describe the use of Kendall’s correlation and Sen’s slope reducer that could investigate the precipitation trends and magnitudes by the processing of TRMM data from 1998 to 2017.They could interpret that “dry gets dryer, wet gets wetter” pattern in the study region. Additionally, they interestingly depicted in an info-graphics showing the distribution of use of GEE deploying regression, machine learning etc. methods in the first paper are as below in figure-2.\nFigure-1: A summary of the algorithms and capabilities available in code editor-Google Earth Engine.\n\n\n\n\n\n\n\n\n\nFigure-2: Categorization of articles that utilized GEE.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>WeekFive: Google Earth Engine (GEE)</span>"
    ]
  },
  {
    "objectID": "WeekFive.html#reflection",
    "href": "WeekFive.html#reflection",
    "title": "7  WeekFive: Google Earth Engine (GEE)",
    "section": "7.3 Reflection",
    "text": "7.3 Reflection\nAs we enter week five, I find myself relieved of the tedious and prolonged application of remote sensing (RS) techniques, as experienced in week 3. The parallel execution of data storage, processing, and application/output delivery in Google Earth Engine (GEE) makes the process convenient, quick, and resourceful. GEE surpasses the earlier methods of iteration and time-consuming processes in earth observation. Its aggregation functions, which provide specific values to each pixel such as median, linear regression, and spatial join, facilitate effective and efficient image processing over massive planetary datasets, making me particularly fond of its capabilities. As someone deeply interested in data, I am fascinated to learn that GEE archives historical imagery and scientific datasets dating back 40 years, including satellite data from sources such as Landsat, Sentinel-2, and MODIS, as well as geophysical, weather, climate, and demographic data. This knowledge has fueled my enthusiasm and engagement in remote sensing and geospatial data science studies.\nHowever, unlike most of the GIS analysis I have learned in ‘R’, utilizing GEE requires knowledge and skills in JavaScript and Python coding. Although we have been provided with codes, deploying them to suit various projects in the future may require debugging and troubleshooting, often leading to time-consuming searches for solutions on online communities such as Stack Exchange. This raises questions about the use of medium spatial resolution imagery available from the European Space Agency, which is provided free of charge, compared to high spatial resolution imagery from commercial satellites that are expensive to purchase. While the free availability of Google’s platform for GEE is a commendable initiative, concerns linger about its sustainability. So far, my experience with the monetarily charged aspects of GEE has been positive.\nIn conclusion, I couldn’t have asked for more than the relief from the multiple runs of imagery pre-processing, processing, classification, accuracy assessments, etc., in remote sensing, which are streamlined with the use of code in GEE. With a basic setup and overview understanding of GEE’s application in RS, I look forward to delving into classification in greater detail in upcoming sessions.\n\n\n\n\nAmani, Meisam, Arsalan Ghorbanian, Seyed Ali Ahmadi, Mohammad Kakooei, Armin Moghimi, S. Mohammad Mirmazloumi, Sayyed Hamed Alizadeh Moghaddam, et al. 2020. “Google Earth Engine Cloud Computing Platform for Remote Sensing Big Data Applications: A Comprehensive Review.” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 13: 5326–50. https://doi.org/10.1109/JSTARS.2020.3021052.\n\n\nTamiminia, Haifa, Bahram Salehi, Masoud Mahdianpari, Lindi Quackenbush, Sarina Adeli, and B. Brisco. 2020a. “Google Earth Engine for Geo-Big Data Applications: A Meta-Analysis and Systematic Review.” ISPRS Journal of Photogrammetry and Remote Sensing, May. https://doi.org/10.1016/j.isprsjprs.2020.04.001.\n\n\n———. 2020b. “Google Earth Engine for Geo-Big Data Applications: A Meta-Analysis and Systematic Review.” ISPRS Journal of Photogrammetry and Remote Sensing, May. https://doi.org/10.1016/j.isprsjprs.2020.04.001.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>WeekFive: Google Earth Engine (GEE)</span>"
    ]
  },
  {
    "objectID": "WeekSix.html#summary",
    "href": "WeekSix.html#summary",
    "title": "8  WeekSix: Classification",
    "section": "",
    "text": "8.1.1 Classification and Regression Tree (CART)\nUnderstanding that “Regression” is also a machine learning (ML) method involves finding a line of best fit between the dependent and independent variables. Classification and Regression Trees (CART) prove to be useful tools for classifying (classification tree) and predicting (regression tree) pixel imagery data (see figure-1). A classification tree is valuable for pattern recognition in discrete categories of datasets such as temperature, rainfall, wind, and saturation, whereas a regression tree is utilized for continuous dependent variables like GCSE scores. However, when linear regression doesn’t fit well, sub-setting the data into smaller chunks using decision trees (with root, branch, and leaves) becomes feasible, a process known as regression tree.\nThese decision trees may delve deeper, with a criterion like Gini impurity used to find the lowest impurity, which wins. Moreover, for numerical value interests, the sections are divided based on thresholds (nodes), and the sum of squared residuals (SSR) is calculated to assess SSR for different thresholds. Setting a minimum number of observations is necessary to prevent overfitting, and starting with the root of the tree with the lowest SSR is recommended. The process is then repeated across all variables (as the root), yielding a numeric value unlike categories in classification trees.\nTo avoid overfitting, the best model should exhibit low bias and low variability (e.g., test and train). This can be achieved by limiting the growth of the tree (e.g., setting a minimum number of pixels in a leaf, often 20) or by pruning, which involves identifying the weakest link. The challenge of decision trees with new data underscores the appropriateness of using the random forest method.\nFigure-1: Classification and Regression Tree (CART)\n\n\n\n\n\n\n\n\n\n(Krzywinski and Altman 2017)\n\n\n8.1.2 Random Forest\nRandom forest (see figure-2) employs the principle that having multiple decision trees is often better than just one. In this method, the concept of bootstrap sampling is implemented, where data is resampled with replacement to make decisions, a process known as “bagging.” Approximately 70% of the training data is utilized in the bootstrap, while the remaining 30% constitutes out-of-bag (OOB) samples, serving as the testing data. This process is repeated for each tree, with each one being trained on a different set of OOB samples. Notably, no pruning is required in random forests, as the majority voting mechanism determines the final decision. Lastly, the proportion of OOB samples that are incorrectly classified constitutes the out-of-bag error (OOBE). However, a challenge arises with the validation data, which differs from the OOB data and is never included in the decision trees.\nFigure-2 Random Forest Analysis\n\n\n\n\n\n\n\n\n\n(“Random Forest,” n.d.)\n\n\n8.1.3 Working with Imagery\nInsights into the know-how of supervised and unsupervised classification of pixel data in remote sensing (RS) are crucial for understanding their application on imagery. As machine learning has progressed, there are both generic machine learning algorithms and those specific to remote sensing.\nIn supervised classification, pixels are not treated in isolation but rather in context with neighboring pixels and objects (polygons), considering factors such as texture. Unsupervised classification operates through clustering algorithms like k-means and DBSCAN. The iterative self-organizing data analysis technique (ISODATA) algorithm, used for multispectral pattern recognition, performs cluster busting.\nFurthermore, common algorithms learned include decision trees, random forests, maximum likelihood, and support vector machines (SVMs), all significant in supervised classification. Maximum likelihood is a traditional classifier where prior probability information, such as 60% urban, is specified for the data (land cover) likely to have certain values in the pixel. SVM, on the other hand, sets a hyperplane between the maximum margin classifier and support vectors, often employing 3D planes to support more than two datasets.\nFigure-3: CART and Pixel Approach for Delhi, India",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>WeekSix: Classification</span>"
    ]
  },
  {
    "objectID": "WeekSix.html#related-literature-findings",
    "href": "WeekSix.html#related-literature-findings",
    "title": "9  WeekSix: Classification",
    "section": "9.2 Related Literature Findings",
    "text": "9.2 Related Literature Findings\nThe research by Chen, W. et al. carried a comparative assessment of logistic model tree, random forest, and classification and regression tree models for spatial prediction of landslide susceptibility. They found that random forest model showed promising application with success rate of 0.837 and a prediction rate of 0.781 than CART incase of landslide susceptibility mapping in Long County, Georgia. However, a study by team of Mohajane, M. also found the RF (RF-FR) achieved the highest performance (AUC = 0.989) that out performed other methods but interestingly established SVM (AUC = 0.959) to be next supportive after RF, better than CART (AUC = 0.847) in forecasting of the forest fire. The former study applies linear-support vector machine algorithm (L-SVM) to evaluate the predictive capability of the 12 landslide conditioning factors. Whereas, the later researches about the developes Frequency Ratio-Support Vector Machine (FR-SVM) model as one of the hybrid machine learning algorithms to compare with other four i.e., Frequency Ratio-Multilayer Perceptron (FR-MLP), Frequency Ratio-Logistic Regression (FR-LR), Frequency Ratio-Classification and Regression Tree (FR-CART) and Frequency Ratio-Random Forest (FR-RF), for mapping forest fire susceptibility in the north of Morocco. The first study considered twelve landslide-related parameters, including slope angle, slope aspect, plan curvature, profile curvature, altitude, NDVI, land use, distance to faults, distance to roads, distance to rivers, lithology, and rainfall. On the other hand, the second study used 10 independent causal factors including elevation, slope, aspect, distance to roads, distance to residential areas, land use, normalized difference vegetation index (NDVI), rainfall, temperature, and wind speed.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>WeekSix: Classification</span>"
    ]
  },
  {
    "objectID": "WeekSix.html#reflection",
    "href": "WeekSix.html#reflection",
    "title": "8  WeekSix: Classification",
    "section": "8.3 Reflection",
    "text": "8.3 Reflection\nAlthough the lecture was long, it was definitely worth the effort to learn about the fascinating processes behind GEE application in RS. I particularly appreciated the opening part where the supervised classification of CART and random forest method was discussed, highlighting how machine learning drives pixel imagery processing in an intuitive manner similar to human learning.\nPreviously intimidating terms like Machine Learning, Cloud Computing, and Deep Learning now seem like exciting avenues for learning, exploration, and application. It was reassuring to hear from the professor that there are only a few lines of code needed to perform tasks that would otherwise take numerous slides of lecture material.\nMoreover, relating the concepts covered in the lecture to those learned in CASA0005, such as linear regression, best line-fit, residual assumption, clustering, k-means, and DBSCAN, helped solidify my understanding. Despite initially feeling apprehensive about not being as practiced in GIS concepts or R-studio, everything began to fall into place as the lecture progressed.\nAs we delve deeper into pixel imagery data processing, I’m intrigued by the upcoming discussion on the identification and processing of hard or soft surfaces and the choice between pixel or object methods. It’s exciting to see what else we’ll uncover in the sessions to come!\n\n\n\n\nChen, Wei, Xiaoshen Xie, Jiale Wang, Biswajeet Pradhan, Haoyuan Hong, Dieu Tien Bui, Zhao Duan, and Jianquan Ma. 2017. “A Comparative Study of Logistic Model Tree, Random Forest, and Classification and Regression Tree Models for Spatial Prediction of Landslide Susceptibility.” CATENA 151 (April): 147–60. https://doi.org/10.1016/j.catena.2016.11.032.\n\n\nKrzywinski, Martin, and Naomi Altman. 2017. “Classification and Regression Trees.” Nature Methods 14 (8): 757–58. https://doi.org/10.1038/nmeth.4370.\n\n\nMohajane, Meriame, Romulus Costache, Firoozeh Karimi, Quoc Bao Pham, Ali Essahlaoui, Hoang Nguyen, Giovanni Laneve, and Fatiha Oudija. 2021. “Application of Remote Sensing and Machine Learning Algorithms for Forest Fire Mapping in a Mediterranean Area.” Ecological Indicators 129 (October): 107869. https://doi.org/10.1016/j.ecolind.2021.107869.\n\n\n“Random Forest.” n.d. https://corporatefinanceinstitute.com/resources/data-science/random-forest/.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>WeekSix: Classification</span>"
    ]
  },
  {
    "objectID": "WeekSeven.html#summary",
    "href": "WeekSeven.html#summary",
    "title": "9  WeekSeven: Classification-2 The Big Question",
    "section": "",
    "text": "9.1.1 Object Based Image Analysis (OBIA)\nObject-Based Image Analysis (OBIA) is an approach to image analysis that considers raster cells as part of objects on the ground rather than treating each cell independently. A raster cell does not directly represent an object on the ground but objects are represented by shapes based on the similarity (homogeneity) or difference (heterogeneity) of the cells, which are grouped into superpixels. The most common method for generating superpixels is the SLIC (Simple Linear Iterative Clustering) Algorithm. This algorithm works by placing regular points on the image and computing spatial distance and color difference to group pixels into homogeneous regions. Tools like Supercell’s package facilitate the generation of superpixels and allow for customization of parameters such as the number of superpixels (K) and compactness, which balances spatial and color considerations. Before classification, features are often transformed, typically into the LAB color space, to enhance discrimination between objects based on color and spatial characteristics. After feature extraction, average values per object are computed, and classification methods similar to those used in pixel-based analysis can be applied to classify objects.OBIA allows for the computation of various metrics beyond average values, such as length-to-width ratio, to further characterize objects using additional metrics. There are several OBIA classifiers available, each employing slightly different processes. It’s essential to choose a classifier that best suits the specific analysis needs and data characteristics. More advanced packages like SegOptim offer algorithms from other software for enhanced object-based analysis capabilities.\nFigure-1: Object Based Image Analysis (OBIA)\n\n\n\n\n\n\n\n\n\n(“Image Classification & Object Based Image Analysis (OBIA) - Home - Aerial/Satellite Digital Mapping Solutions - LAND INFO ... Landinfo.com” 2020)\n\n\n9.1.2 Sub-Pixel Analysis\nSubpixel analysis, also known as Spectral Mixture Analysis (SMA) or Linear Spectral Unmixing, is a method used to determine the proportions or abundances of different land cover types within a single pixel. This analysis calculates the proportion or abundance of different land cover types within a pixel based on the assumption that the reflectance measured at each pixel is a linear combination of endmembers weighted by their associated fractions. Endmembers are spectrally pure representations of different land cover types. Typically, a few endmembers are selected to represent the variability in spectral signatures across the image. The abundance of each endmember within a pixel is determined by multiplying the reflectance of each endmember by its fraction contribution to the best-fit mixed spectrum. It has few issues and consideration as pixel purity, number of endmembers and multiple endmember spectral analysis (MESMA). In pixel purity the subpixel analysis assumes that each pixel contains a mixture of endmembers, but in reality, pixels may contain spectrally pure or mixed endmembers, impacting the accuracy of the analysis. In the case of the number of endmembers, the selection of endmembers is crucial and depends on the complexity of the landscape. In urban areas, the V-I-S model (Vegetation-Impervious surface-Soil) is commonly used to simplify the process. Multiple Endmember Spectral Analysis (MESMA), allows for the use of multiple endmembers to better capture the spectral variability within a pixel. However, this approach increases computational complexity, and spectral libraries may be used to aid in endmember selection.Thus, careful consideration of pixel purity and endmember selection is necessary to ensure accurate results.\nFigure-2: Sub-Pixel Analysis\n\n\n\n\n\n\n\n\n\n(Perikamana, Balakrishnan, and Tripathy 2021)\n\n\n9.1.3 Accuracy\nAccuracy assessment in remote sensing involves assigning accuracy values to classification results, similar to the process in machine learning. Key metrics include Producer Accuracy (PA), User’s Accuracy (UA), and Overall Accuracy (OA). Producer Accuracy (PA), also known as recall or true positive rate, measures the vertical accuracy of classification results. It assesses how well the classification results meet the expectations of the data creator. User’s Accuracy (UA), also referred to as precision or positive predictive value, evaluates the horizontal accuracy of classification results. It measures the proportion of pixels that are correctly classified as a known class out of all pixels classified as that class. Overall Accuracy (OA) represents the combined fraction of correctly classified pixels across all land cover types.\nIn assessing accuracy, several terms are used to categorize correct and incorrect classifications:\nTrue Positive (TP): The model predicts the positive class correctly. True Negative (TN): The model predicts the negative class correctly. False Positive (FP): The model predicts positive, but it is negative. False Negative (FN): The model predicts negative, but it is positive. Producer’s and User’s Accuracy are defined as follows:\nProducer’s Accuracy: TP / (TP + FN) User’s Accuracy: TP / (TP + FP)\nErrors of omission and commission are also considered. Errors of omission occur when a land cover type is omitted from the correct class. It is calculated as 100 - Producer’s Accuracy. In contrast, errors of commission involve classified sites for incorrect classifications. It is computed as 100 - User’s Accuracy. Additionally, the Kappa coefficient is used to assess the agreement between classification results and ground truth data.\nBeyond traditional remote sensing accuracy assessment, considerations include the balance between recall (Producer Accuracy) and precision (User Accuracy). The Receiver Operating Characteristic (ROC) Curve and the Area Under the ROC Curve (AUC) are employed to evaluate model performance, especially in binary classification tasks. The ROC Curve plots the true positive rate against the false positive rate, while the AUC represents the probability that the model will rank a randomly chosen positive example higher than a randomly chosen negative example. A higher AUC indicates better model performance, with a perfect classifier achieving an AUC of 1.\n\n9.1.3.1 Train and Test Split\nA good approach in machine learning is to split the dataset into training and testing sets. This involves reserving a certain percentage of the original data for training the model and keeping the remaining portion for testing the model’s performance. The training set is used to train the model’s parameters, while the testing set evaluates the model’s performance on unseen data.\n\n\n9.1.3.2 Leave-One-Out Cross-Validation (LOOCV)\nLOOCV is an extreme form of cross-validation suitable for smaller datasets. In LOOCV, each data point is used as the testing set once, while the rest of the data is used for training. This process is repeated for each data point, resulting in multiple rounds of training and testing.\n\n\n9.1.3.3 Spatial Cross-Validation\nSpatial cross-validation is a variation of traditional cross-validation that considers spatial relationships in the dataset. It involves partitioning the data into folds while ensuring spatial disjointness, typically using techniques like k-means clustering based on the number of points and a distance metric. This ensures that the training and testing data are not spatially correlated, preventing the model from overfitting to local spatial patterns. The process is illustrated in figure-3 for Delhi.\nFigure-3: Accuracy, OBIA, Super-pixel Analysis for Delhi, India",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>WeekSeven: Classification-2 The Big Question</span>"
    ]
  },
  {
    "objectID": "WeekSeven.html#literature-review",
    "href": "WeekSeven.html#literature-review",
    "title": "10  WeekSeven: Classification-2 The Big Question",
    "section": "10.2 Literature Review",
    "text": "10.2 Literature Review\nResearch conducted by Saba, S.B. et al. explains how timely and exact delineation of landslide is imperative to decision makers for for effective and quick response to such disasters. Their team describe the virtues and drawbacks of pixel, sub-pixel, and Object-Based Image Analysis (OBIA)-based approach for local to regional area, having the problem of accuracy to extract the regional level auxilluary datasets. Whereas the investigation by Qu, L.A. et al. shows concern about the extracting accuracy issue of Land Use Land Cover just by using spectral feature of imagery due to heterogeneity in landform for larger area which they attempt to provide better results using open-source datasets in GEE elucidating along with methods of pixel and object based classification. The study area for the first paper is in Muzaffarabad, Pakistan (Lesser Himalayas) that compares the classification methods based on MLC (Maximum Likelihood Classifier), the Co-Registration of Optically Sensed Images and Correlation (COSI-Corr), and the OBIA using SPOT and ASTER imagery. On the other hand the second research does LULC classification using medium resolution imagery from Landsat-8 OLI data for the Yangtze River Delta in China.\nSaba, S.B. et al. carried comparative classification techniques i.e., MLC (Maximum Likelihood Classifier), the Co-Registration of Optically Sensed Images and Correlation (COSI-Corr), and the OBIA using SPOT and ASTER imagery. Qu, L.A. et al. compared the improvement of the accuracy of seven pixels-based and seven object-based random forest classification models. Where Saba, S.B. et al. commented about the overall accuracy improvement of the classification for landslide features using satellite data, Qu, L.A. et al. discovered that the overall accuracy of the classification of Land Use Land Cover using GEE auxiliary datasets can be improved irrespective of types of auxiliary features. Saba, S.B. et al.found that overall accuracy of the OBIA classification depicted the top result with 91.4%, followed by sub-pixel COSI-Corr having 90.9 % and with 80.8 % for pixel-based classification accuracy. Similarly, Qu, L.A. et al. confirmed that the object-based classification achieves higher overall accuracy compared to that obtained by the pixel-based classification.\nFormer group of authors also found that the OBIA result is more spatially consistent than the pixel-based outcomes with speckled pixel effects, and depending on visual interpretation. The later said that the topographic features play the most important role in improving the overall accuracy of classification in the pixel- and object-based models comprising all features. Further, a higher accuracy is achieved when the object-based method is used with only spectral data, small objects on the ground cannot be monitored. However, combined with many types of auxiliary features, the object-based method can identify small objects while also achieving greater accuracy.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>WeekSeven: Classification-2 The Big Question</span>"
    ]
  },
  {
    "objectID": "WeekSeven.html#reflection",
    "href": "WeekSeven.html#reflection",
    "title": "9  WeekSeven: Classification-2 The Big Question",
    "section": "9.3 Reflection",
    "text": "9.3 Reflection\nOBIA, or Object-Based Image Analysis, revolutionizes image analysis by treating raster cells as components of larger, coherent objects on the ground. This methodology goes beyond traditional pixel-based analysis by considering not only individual pixel values but also the spatial relationships and spectral characteristics of neighboring pixels.\nIn OBIA, the process typically begins with the generation of superpixels, which are clusters of pixels with similar spectral and spatial properties. These superpixels are then subjected to feature transformation, where various attributes such as color, texture, shape, and size are extracted to characterize the objects more comprehensively. Finally, classification techniques are applied to these transformed features to categorize the objects into meaningful classes or categories.\nUnlike pixel-based analysis, which evaluates each pixel independently, OBIA groups pixels into objects based on their contextual information, leading to more accurate and semantically meaningful results. By considering the spatial context and spectral properties of objects, OBIA can capture complex patterns and structures in remote sensing data more effectively.\nIn summary, OBIA offers a powerful framework for extracting valuable insights from remote sensing imagery by leveraging spatial and spectral information to delineate and classify objects on the ground. Its ability to analyze imagery at the object level provides a more holistic understanding of the landscape, making it an indispensable tool for a wide range of applications in environmental monitoring, land cover classification, urban planning, and more.\n\n\n\n\n“Image Classification & Object Based Image Analysis (OBIA) - Home - Aerial/Satellite Digital Mapping Solutions - LAND INFO ... Landinfo.com.” 2020. https://landinfo.com/image-classification-object-based-image-analysis-obia/.\n\n\nPerikamana, Krishna Kumar, Krishnachandran Balakrishnan, and Pratyush Tripathy. 2021. A CNN Based Method for Sub-Pixel Urban Land Cover Classification Using Landsat-5 TM and Resourcesat-1 LISS-IV Imagery.\n\n\nQu, Le’an, Zhenjie Chen, Manchun Li, Junjun Zhi, and Huiming Wang. 2021. “Accuracy Improvements to Pixel-Based and Object-Based LULC Classification with Auxiliary Datasets from Google Earth Engine.” Remote Sensing 13 (3): 453. https://doi.org/10.3390/rs13030453.\n\n\nSaba, Sumbal, Muhammad Ali, Syed Ali Turab, Muhammad Waseem, and Shah Faisal. 2022. “Comparison of Pixel, Sub-Pixel and Object-Based Image Analysis Techniques for Co-Seismic Landslides Detection in Seismically Active Area in Lesser Himalaya, Pakistan.” Natural Hazards 114 (October). https://doi.org/10.1007/s11069-022-05642-y.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>WeekSeven: Classification-2 The Big Question</span>"
    ]
  },
  {
    "objectID": "WeekFive.html#related-literature-findings",
    "href": "WeekFive.html#related-literature-findings",
    "title": "8  WeekFive: Google Earth Engine (GEE)",
    "section": "8.2 Related Literature Findings",
    "text": "8.2 Related Literature Findings\nWhen reviewed in recent research articles, it is evident both the study by Tamiminia, H. et al. and Amani, M. et al. inferred that the use of GEE is still evolving in the field of remote sensing ever since its launch in 2010. However, Tamiminia, H. et al. have brought a summary of algorithms and capabilities in term of packages that are available, such as machine learning, image processing, image collection, geometry-feature, reducer, charts and specialized algorithms to explore the setup of GEE application in RS (refer figure-1). On the other hand, the team of Amani, M. et al. describes the application of GEE that was used along with version 1 Tropical Rainfall Measuring Mission (TRMM) precipitation products to study the spatial and temporal patterns of precipitation in the Zambezi River basin. They further describe the use of Kendall’s correlation and Sen’s slope reducer that could investigate the precipitation trends and magnitudes by the processing of TRMM data from 1998 to 2017.They could interpret that “dry gets dryer, wet gets wetter” pattern in the study region. Additionally, they interestingly depicted in an info-graphics showing the distribution of use of GEE deploying regression, machine learning etc. methods in the first paper are as below in figure-2.\nFigure-1: A summary of the algorithms and capabilities available in code editor-Google Earth Engine.\n\n\n\n\n\n\n\n\n\nFigure-2: Categorization of articles that utilized GEE.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>WeekFive: Google Earth Engine (GEE)</span>"
    ]
  },
  {
    "objectID": "WeekEight.html#summary",
    "href": "WeekEight.html#summary",
    "title": "10  WeekEight: Temperature",
    "section": "",
    "text": "10.1.1 SAR fundamentals\nSynthetic Aperture Radar called as SAR are active sensors that can see through weather and clouds and sense surface texture. Its applications vary as per the wavelengths. SAR technology records the backscattered signal when it emits electromagnetic signals to generate images and its data. SAR is appreciated for higher resolution imagery which is because its radar emits signals while moving forward creating a longer antenna aperture in the azimuth direction. While moving it takes multiple images pixels while sweeping the footprint or swath. The advantage of such movement gets compensated by combining the received signals to synthesize a longer antenna aperture.\nOrientation of the electromagnetic wave as transmitted by the radar refers to SAR polarization (figure-2) have a single polarization that emits and receives either horizontal (H) or vertical (V) polarization signals as well as dual one can transmits and receives bot both polarizations (HH and VV). Similarly, varying surfaces respond differently to polarizations such as rough surfaces can be sensed as VV polarization, volume scattering surfaces to cross-polarizations (VH or HV), and double bounce surfaces to HH polarization can be done because SAR signals have amplitude (backscatter) and phase difference data (in figure-1). An interferometric SAR (InSAR) uses phase data in order to detect ground movement, with changes in phase indicating surface displacement and its multiple images are used in surface topography or motion. Additionally, DInSAR (differential InSAR) allows elimination of natural elevation on phase shift for accurate detection of surface movement etc.\nFigure-1: SAR Movement (backscattering and phase difference)\n\n\n\n\n\n\n\n\n\n(Perdikou and Kouhartsiouk 2022)\nFigure-2: Polarisation\n\n\n\n\n\n\n\n\n\n(“This is the forefront of the SAR image! Viewing images taken by ICEYE on Tellus!” n.d.)\n\n\n10.1.2 Change detection with SAR\nTo detect change via SAR data, brings multiple techniques suitable for readily statistical application on its imagery. In addition, its advantage of calibration errors make it fit for methods such as ratio images, improved ratio images, log ratio images etc. rather than simple methods as in case of optical data. Statistical tests such as standard deviation over time, with higher standard deviation indicating change can assess pixel wise change. Similarly, image collections analysis can deliver the change detection. The Receiver Operating Characteristic (AUC-ROC) curves may change detection accuracy via changing threshold classification. This can be done by maximizing true positives vs minimizing false positives.\nSAR data can be fused with optical imagery at decision level fusion, object level fusion, and image fusion known as Image fusion technique. In decision level fusion SAR and optical bands are combined in different layers to classify the combined data whereas with object level fusion, the SAR and optical data derive new pixel values to detect change. Thus, are robust methods for change detection accuracy (see figure-3).\nFigure-3: Accuracy (True-Positive, True-Negative, False-Positive, False-Negative)\n\n\n\n\n\n\n\n\n\n(Lopez-de-la-Calleja et al. 2013)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>WeekEight: Temperature</span>"
    ]
  },
  {
    "objectID": "WeekEight.html#application",
    "href": "WeekEight.html#application",
    "title": "10  WeekEight: Temperature",
    "section": "10.2 Application",
    "text": "10.2 Application\nMany researchers exist on the temporal interferometric synthetic aperture radar (InSAR). (Pelich et al. 2022) emphasised that most research exists on single co-polarization images for flood water studies in order to identify with the double-bounce feature. However, their study added the cross-polarization imaging to enhance the mapping of urban flood water through multi-temporal InSAR coherence. Thus, in applying cross-polarisation, their study methodology brought an accuracy increase in maps of urban flood from 75.2% by using 82.9 %.\nWhereas in the landslide susceptibility research of (Beker et al. 2023) the accuracy could be bettered further with highest precision of AUC value of 98.4%. Moreover, their study shows excellent statistics such as the 93.7 % of ACC value, 87.4% of a KAPPA value and 87.5% of the MCC value. Their model performance evaluations were developed using area under the curve (AUC), accuracy (ACC), kappa coefficient (KAPPA), and the Matthews correlation coefficient (MCC). In addition, they got high resultant when compared to CNN-support vector machine (CNN-SVM), CNN-random forest (CNN-RF), and CNN-logistic regression (CNN-LR).\nIn another disaster study of volcanic events, InSAR has been extensively used by Beker, T. et al. with five year data stacks by training the models on a synthetic training set using the confusion matrix. Artificial intelligence (AI) analyses improved the model where false-positive (figure-4) detections aided to link slope processes and salt lake deformations patterns to Gradient-weighted Class Activation Mapping (Grad-CAM). Additionally, model betterment via hybrid synthetic-real data gave more performance extraction using low-pass spatial filtering (LSF) with the real test set. Later, hidden feature visualisation by t-distributed stochastic neighbor embedding (t-SNE) provided lacuna of FT set. Thus, it brought the problem of elevation parameters. Detection included small volcanic deformations with all the others varying from 5 mm/year to 9.9-17.5 mm/year. were done in.\nFigure-4: Comparison of the ROCs on the real test set of models using the FT approach. InceptionResNet v2 architecture demonstrates significantly better performance in comparison to other tested models.\n\n\n\n\n\n\n\n\n\n(Beker et al. 2023)",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>WeekEight: Temperature</span>"
    ]
  },
  {
    "objectID": "WeekEight.html#reflection",
    "href": "WeekEight.html#reflection",
    "title": "10  WeekEight: Temperature",
    "section": "10.3 Reflection",
    "text": "10.3 Reflection\nSynthetic Aperture Radar (SAR) operates as an active sensor, emitting electromagnetic signals and recording the amplitude (backscatter) and phase data. Key considerations in SAR data interpretation include polarization, which affects how surfaces respond to the radar signal, and permittivity, indicating the reflective properties of materials. The wavelength (or band) of SAR signals influences the type of information captured. In Google Earth Engine (GEE), SAR data typically comprises amplitude (backscatter) information, lacking phase data. SAR data in GEE is presented in three units: power scale (RAW SAR) for statistical analysis, amplitude for visualization purposes, and decibel (dB) scale for detecting differences, which is the default scale in GEE.\nIt is interesting to know that in GEE, only amplitude (backscatter) data is available but to use phase data we need to use SNAP (not considered here), one need to understand that What are we trying to detect, roughness or volume of a material such as polarization, type of material and part of earth (e.g. water).\n\n\n\n\nBeker, Teo, Homa Ansari, Sina Montazeri, Qian Song, and Xiao Xiang Zhu. 2023. “Deep Learning for Subtle Volcanic Deformation Detection with InSAR Data in Central Volcanic Zone.” IEEE Transactions on Geoscience and Remote Sensing 61: 1–20. https://doi.org/10.1109/TGRS.2023.3318469.\n\n\nLopez-de-la-Calleja, Miriam, Takayuki Nagai, Muhammad Attamimi, M. Nakano-Miyatake, and Hector Perez-Meana. 2013. “Object Detection Using SURF and Superpixels.” Journal of Software Engineering and Applications 06 (January): 511–18. https://doi.org/10.4236/jsea.2013.69061.\n\n\nPelich, Ramona, Marco Chini, Renaud Hostache, Patrick Matgen, Luca Pulvirenti, and Nazzareno Pierdicca. 2022. “Mapping Floods in Urban Areas from Dual-Polarization InSAR Coherence Data.” IEEE Geoscience and Remote Sensing Letters 19: 1–5. https://doi.org/10.1109/LGRS.2021.3110132.\n\n\nPerdikou, Skevi, and Demetris Kouhartsiouk. 2022. “Condition Assessment of Levees and Embankments via Satellite Radar Interferometry.” In.\n\n\n“This is the forefront of the SAR image! Viewing images taken by ICEYE on Tellus!” n.d. https://sorabatake.jp/en/15537/.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>WeekEight: Temperature</span>"
    ]
  },
  {
    "objectID": "WeekFFour.html",
    "href": "WeekFFour.html",
    "title": "7  WeekFour: Policy and Remote Sensing",
    "section": "",
    "text": "7.1 Summary",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>WeekFour: Policy and Remote Sensing</span>"
    ]
  },
  {
    "objectID": "WeekFFour.html#summary",
    "href": "WeekFFour.html#summary",
    "title": "7  WeekFour: Policy and Remote Sensing",
    "section": "",
    "text": "7.1.1 Policy Application and Earth Observation Data\nThere exists a large variety of EO data at disposal. These are data from the sensors for remote sensing application that depends on a the combination of spectral bands, resolutions and the cost. For example multi-temporal land cover (or land use) mapping, spectral signatures or libraries, change detection for e.g. urban or forest, vegetation stress for illegal logging, precipitation, elevation models (or point data) - such as LiDAR, temperature, night time lights (urban development), forest fire monitoring / predicting / “hot spot” detecting, pollution monitoring, drought indices, informal housing detection, water level data for monitoring, building or network outline (polygon / line) extraction, environmental monitoring (e.g. Aral Sea), estimations of resources - forest, water, snow, ice, green space\n\n\n7.1.2 Challenges in the State-of-Art-of-Policy-Making\nThe global policy documents says that the New Urban Agenda is about standards and principles for planning, construction, development, management and urban improvement. However, its subsection 64, 65 and 67 only states the aspects of concern for particularly affect coastal areas, delta regions and small island developing States, among others. Discussion on the Sustainable Development Goals (SDG) that targets with measurable indicators for monitoring where Goal 11 of make cities and human settlements inclusive, safe, resilient and sustainable. It talks about target 11.5, monitoring 11.5 and data 11.5 with some understanding of data and its measuring ability having a data collection framework in disaster studies. However, some new addition of RS and EO data gets enclosed in targets 11.6 and 11.7. A complex approach that is difficult to discern on Who are the targeted community, its replication, and usefulness of the results. Metropolitan policy documents of London, NYC, Cape Town and Ahmadabad were also difficult to provide data specifications, their use and applicability. Similarly the local level planning and development authorities do work on fire, policing, transport and development guidelines and have to adheres to these goals too. It can be concluded that some studies fail to integrate their outputs with requirements of local areas, governments or cities with generic statements. In addition, cities set policies but often focus on legislation or monitoring as opposed to prevention.\nMoreover usage of EO data are lost in these efforts. Therefore, the informed EO data policy and decision-making can help achieve their goals and improve urban areas more effectively in a data informed manner. The professor grouped us into groups to present with a case study to pitch the concern government the idea of one/ more suitable remote sensing application, data and detail to resolve a pressing matter in their metropolitan area level.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>WeekFour: Policy and Remote Sensing</span>"
    ]
  },
  {
    "objectID": "WeekFFour.html#application",
    "href": "WeekFFour.html#application",
    "title": "7  WeekFour: Policy and Remote Sensing",
    "section": "7.2 Application",
    "text": "7.2 Application\nThe authors K.S.S. Parthasarathy and Paresh Chandra Deka in their article, review on remote sensing and GIS application in assessment of coastal vulnerability and shoreline changes. They review and assess the shoreline changes and the coastal vulnerability index (CVI) providing the insight knowledge for the literature to determine the critical parameter that influences the coast to a greater extent for a particular study. However, the review article they miss on the application of RS aid to regional bodies to mitigate coastal vulnerability and shoreline change. Interestingly, K. Sowmya et al. studies urban flood vulnerability zoning of Cochin City, southwest coast of India, using remote sensing along with GIS tool. Assessing the vulnerability through the application of multi-criteria evaluation approach in a GIS environment using remotely sensed images along with SRTM DEM, census details, city maps and field study, they They prepare a flood vulnerability zoning into categories of low, moderate, high and very high vulnerable areas. In turn they calculate the proportion of area under very high and high category and nature of vulnerability. However, their effort leaves the reader to wonder how to implement the RS and GIS driven study at the spatial level.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>WeekFour: Policy and Remote Sensing</span>"
    ]
  },
  {
    "objectID": "WeekFFour.html#reflection",
    "href": "WeekFFour.html#reflection",
    "title": "7  WeekFour: Policy and Remote Sensing",
    "section": "7.3 Reflection",
    "text": "7.3 Reflection\nI liked the part where we carry forward the learning of last term module of CASA0005-GIS and apply linear regression, spatial auto regression (SAR), lag etc. RS standalone cannot help but needs the GIS tools too. There exists dis-integrated effort in term of tool and techniques to resolve spatial problems which the articles refereed in the application part represents too. The details discussed on different sensors data stood out for me. Additionally, we added InSAR learning to last week’s SAR knowledge.\nReference: Parthasarathy, K.S.S. and Deka, P.C., 2021. Remote sensing and GIS application in assessment of coastal vulnerability and shoreline changes: a review. ISH Journal of Hydraulic Engineering, 27(sup1), pp.588-600. Sowmya, K., John, C. M., & Shrivasthava, N. K. (2015). Urban flood vulnerability zoning of Cochin City, southwest coast of India, using remote sensing and GIS. Natural Hazards, 75, 1271-1286.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>WeekFour: Policy and Remote Sensing</span>"
    ]
  },
  {
    "objectID": "WeekOne.html#application",
    "href": "WeekOne.html#application",
    "title": "3  Week One: Introduction and Background",
    "section": "3.2 Application",
    "text": "3.2 Application\nIn remote sensing basics by (Santosh and Sundaresan 2014) have described procedures and principles of collect data through remotely sensed methods. They have discussed the components of remote sensing for instance electromagnetic radiation, energy transmission, radiance flux, and the scattering of electromagnetic radiation. Further, they have described active and passive satellites for remote sensing. While explaining the electromagnetic radiation as atmosphere window or regions of atmosphere is significant to develop remote sensing technology. They continuation to emphasis that in order to study any natural process, wavelength region is importation for image identification and classification remote sensing. Moreover, they explain radiometric processing, image classification, and image interpretation techniques to effectively process and understand remote sensing digital data so as to enrich geo-spatial technology.\nIn another study on basics of remote sensing for hydrocarbon identification (Laake 2022), a brief history of the development, implementation and utilization of remote sensing has been done. They conclude on basics of electromagnetic waves crossing atmosphere and in water and soil along with their impact on the spectra useable for remote sensing. Depending on the need, it may focus on earth surface, landuse or lithological application. In addition, shallow water and hot arid deserts may be done by sub surface mapping in special cases.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week One: Introduction and Background</span>"
    ]
  },
  {
    "objectID": "WeekOne.html#reflection",
    "href": "WeekOne.html#reflection",
    "title": "3  Week One: Introduction and Background",
    "section": "3.3 Reflection",
    "text": "3.3 Reflection\nToday is day one of our lecture and only know that this lecture is on remote sensing which is something to do with satellites and data. While it has begun, I am stressed as to if this module is gonna be a tough one because I am overwhelmed by my only little knowledge of the term associated with it i.e., “satellite”, “scientists” and most of all “NASA”. It kind of give me a fear of learning as it meant for strong brainiest and brainwaves. The first lecture is over and I feel little better about the above terms as the prof make is quite engaging to understand and breaks it down precisely for my short wavelength. I like the fun facts stated during the lecture for example there are about 27,000 pieces of junks/ waste satellites lying in out in space. What actually resonates mostly for me that in upcoming lectures and practicals one would learn to enable skills that reduces the time of data imaging via use of Google earth engine (GEE) that gives more free resources to process large volumes of data with the advent of cloud computing. With rising demand of earth observation data and analysis there seems potential emergence of policy decisions to make use of remotely sensed data. However, it is filled with jargon that often isn’t as complicated as it sounds. Thus, I look forward to reach the week of GEE but it might be too ambitious for me as of now. So, I am gonna focus week by week.\n.footnote[  [1] NASA launched Landsat satellites that changed the method of collecting earth’s landscape data popularly known as remote sensing which interchangeably called as ‘Earth Observation’. [2] Scientist started to get more data and details as the NASA went ahead with launches of satellites from Landsat1 to Landsat8. So far to receive enormous detailed data about earth’s resources and climate, the best has been Landsat8, with more sensitive sensor scientist can retrieve improved accuracy in data that helps to manage the earth’s resources and climate optimally. For example, study to most subtle change is vegetation is possible now with Landsat8. [3] Sensors are mounted on satellites, drones etc. The sensors monitor electromagnetic rays of spectrum. They are of two types. First, passive sensors are those that does not emit energy but uses the energy that is available i.e., sun. Second, active sensors are the once that emit energy. [4] It is around us all the time, without which the world around us would not exist is the Electromagnetic Radiations. These waves spread across from a very short radiations of gamma rays, x-rays, ultra-violate rays, visible-light waves, infrared waves, micro waves, radio waves. It is collectively known as the electromagnetic Spectrum or EMS. [5] Waves of an electromagnetic field, travel through space and carry radiant energy. Waves are part of the EMR spectrum. [6] Energy carried by EMR waves. [7] Energy per unit of time. [8] Energy from the sun. [9] Energy (solar power) from the sun per unit area per unit time (from electromagnetic radiation) [10] Means time here [11] Energy leaving a surface per unit area per unit time [12] Particles are very small compared to the wavelength [13] Particles are the same size compared to the wavelength [14] Particles are much larger than the wavelength [15] Active sensor such as Synthetic Aperture Radar (SAR) can see through clouds. [16] In the majority of cases remotely sensed data is raster. Data format is generally geo.tif. Raster data are stacked vertically in B1, B2, B3 etc.(insert img). LiDAR data points has x, y with the z-dimension to collect attitude and 3D analysis. [17] Spatial resolution has the size of the raster grid per pixel (e.g. 20cm or 30m). One can compare the spatial resolution of 10 by 10 cm vs 1 by 1 km. [18] It means to observe part of spectrum in the window. Spectral Signature is unique signature given to material on earth or its combination shown through a graph. [19] It identifies differences in light or reflectance, in practice this is the range of possible values. The ability of a sensor to identify and show small differences in energy. The higher the resolution, the more sensitive. [20] This is the resolution means the time it revisits (e.g. daily, every 7 days, on demand). Its low resolution means it has a large pixel size (e.g. MODIS is 500m by 500m pixel). [21] specific range of EM spectrum [22] means Bands: range of spectrum, where richer the band, will have more elements to identify. [23] The 7-12 bands [24] 10-25 bands that has continuous spectrum [25] These are pure white reference panels that needs to be calibrated]\n\n\n\n\nAbdelhamid, Ghassan. n.d. “BASIC REMOTE SENSING TRAINING OF NON-GRADUATE IMAGERY USERS “AN EXPERIENCE FROM UNITED ARAB EMIRATES, AIR FORCE GROUND ….”\n\n\nLaake, Andreas. 2022. “Basics of Remote Sensing.” In, edited by Andreas Laake, 3–20. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-73319-3_1.\n\n\n“Remote Sensing - Components, Types, Working and Applications.” 2023. https://www.geeksforgeeks.org/remote-sensing/.\n\n\nSantosh, K. M., and J. Sundaresan. 2014. “Remote Sensing Basics.” In, edited by Janardhanan Sundaresan, K M Santosh, Andrea Déri, Rob Roggema, and Ramesh Singh, 279–90. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-01689-4_17.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week One: Introduction and Background</span>"
    ]
  },
  {
    "objectID": "WeekTwo.html#refelection",
    "href": "WeekTwo.html#refelection",
    "title": "4  Week Two",
    "section": "4.2 Refelection",
    "text": "4.2 Refelection\nIt was interesting to learn how use tools for learning dairy: 1. Xaringan (group presentation) and 2. Quarto (for individual).\n\n\n\n\nGrolemund, Yihui Xie, J. J. Allaire, Garrett. n.d. Chapter 7 Xaringan Presentations | r Markdown: The Definitive Guide. https://bookdown.org/yihui/rmarkdown/xaringan.html.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week Two</span>"
    ]
  },
  {
    "objectID": "WeekTwo.html#reflection",
    "href": "WeekTwo.html#reflection",
    "title": "4  Week Two",
    "section": "4.2 Reflection",
    "text": "4.2 Reflection\nIt was interesting to learn how use tools for learning dairy: 1. Xaringan (group presentation) and 2. Quarto (for individual).\n\n\n\n\nGrolemund, Yihui Xie, J. J. Allaire, Garrett. n.d. Chapter 7 Xaringan Presentations | r Markdown: The Definitive Guide. https://bookdown.org/yihui/rmarkdown/xaringan.html.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week Two</span>"
    ]
  },
  {
    "objectID": "WeekFive.html#application",
    "href": "WeekFive.html#application",
    "title": "7  WeekFive: Google Earth Engine (GEE)",
    "section": "7.2 Application",
    "text": "7.2 Application\nRecent research articles, such as those by (Tamiminia et al. 2020a), underscore the evolving role of Google Earth Engine (GEE) in the field of remote sensing since its launch in 2010. Tamiminia, H. et al. provide a comprehensive overview of algorithms and capabilities available in GEE, including machine learning, image processing, image collection, geometry features, reducers, charts, and specialized algorithms, outlining the potential applications of GEE in remote sensing (see Figure 1). Conversely, (Amani et al. 2020) present a study wherein GEE was utilized in conjunction with version 1 of the Tropical Rainfall Measuring Mission (TRMM) precipitation products to analyze the spatial and temporal patterns of precipitation in the Zambezi River basin. They employed Kendall’s correlation and Sen’s slope reducers to investigate precipitation trends and magnitudes, based on TRMM data spanning from 1998 to 2017, revealing a “dry gets drier, wet gets wetter” pattern in the study region. Furthermore, they visually depicted the distribution of GEE usage, employing regression, machine learning, and other methods, as illustrated in figure 2 and figure-3 of their paper.\nFigure-2: A summary of the algorithms and capabilities available in code editor-Google Earth Engine.\n\n\n\n\n\n\n\n\n\n(Tamiminia et al. 2020b)\nFigure-3: Categorization of articles that utilized GEE.\n\n\n\n\n\n\n\n\n\n(Tamiminia et al. 2020b)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>WeekFive: Google Earth Engine (GEE)</span>"
    ]
  },
  {
    "objectID": "WeekSix.html#application",
    "href": "WeekSix.html#application",
    "title": "8  WeekSix: Classification",
    "section": "8.2 Application",
    "text": "8.2 Application\nhe research conducted by (Chen et al. 2017) involved a comparative assessment of logistic model tree, random forest, and classification and regression tree models for spatial prediction of landslide susceptibility. They found that the random forest model showed promising application with a success rate of 0.837 and a prediction rate of 0.781, outperforming CART in landslide susceptibility mapping in Long County, Georgia. However, a study by the team of (Mohajane et al. 2021) also found that RF (RF-FR) achieved the highest performance (AUC = 0.989), surpassing other methods. Interestingly, they established SVM (AUC = 0.959) to be the next supportive model after RF, performing better than CART (AUC = 0.847) in forecasting forest fires.\nIn the former study, the linear-support vector machine algorithm (L-SVM) was applied to evaluate the predictive capability of 12 landslide conditioning factors. In contrast, the latter research developed the Frequency Ratio-Support Vector Machine (FR-SVM) model, one of the hybrid machine learning algorithms, to compare with four others: Frequency Ratio-Multilayer Perceptron (FR-MLP), Frequency Ratio-Logistic Regression (FR-LR), Frequency Ratio-Classification and Regression Tree (FR-CART), and Frequency Ratio-Random Forest (FR-RF), for mapping forest fire susceptibility in the north of Morocco.\nThe first study considered twelve landslide-related parameters, including slope angle, slope aspect, plan curvature, profile curvature, altitude, NDVI, land use, distance to faults, distance to roads, distance to rivers, lithology, and rainfall. On the other hand, the second study used ten independent causal factors, including elevation, slope, aspect, distance to roads, distance to residential areas, land use, normalized difference vegetation index (NDVI), rainfall, temperature, and wind speed.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>WeekSix: Classification</span>"
    ]
  },
  {
    "objectID": "WeekSeven.html#application",
    "href": "WeekSeven.html#application",
    "title": "9  WeekSeven: Classification-2 The Big Question",
    "section": "9.2 Application",
    "text": "9.2 Application\nResearch conducted by (Saba et al. 2022) explains how timely and exact delineation of landslides is imperative for decision-makers to respond effectively and swiftly to such disasters. Their team describes the virtues and drawbacks of pixel, sub-pixel, and Object-Based Image Analysis (OBIA)-based approaches for local to regional areas, highlighting the challenge of accuracy in extracting regional-level auxiliary datasets. Meanwhile, the investigation by (Qu et al. 2021) addresses concerns about the accuracy of extracting Land Use Land Cover (LULC) solely using spectral features of imagery due to heterogeneity in landforms for larger areas. They attempt to provide better results using open-source datasets in GEE, elucidating methods of pixel and object-based classification. The study area for the first paper is in Muzaffarabad, Pakistan (Lesser Himalayas), where they compare classification methods based on MLC (Maximum Likelihood Classifier), the Co-Registration of Optically Sensed Images and Correlation (COSI-Corr), and OBIA using SPOT and ASTER imagery. On the other hand, the second research focuses on LULC classification using medium-resolution imagery from Landsat-8 OLI data for the Yangtze River Delta in China.\nSaba, S.B. et al. employed comparative classification techniques, including MLC, COSI-Corr, and OBIA using SPOT and ASTER imagery. Qu, L.A. et al. compared the improvement in accuracy of seven pixel-based and seven object-based random forest classification models. While Saba, S.B. et al. observed an overall accuracy improvement in landslide feature classification using satellite data, Qu, L.A. et al. discovered that the overall accuracy of LULC classification using GEE auxiliary datasets can be enhanced regardless of the types of auxiliary features.\nSaba, S.B. et al. found that the OBIA classification yielded the highest overall accuracy at 91.4%, followed by sub-pixel COSI-Corr with 90.9%, and pixel-based classification with 80.8% accuracy. Similarly, Qu, L.A. et al. confirmed that object-based classification achieved higher overall accuracy compared to pixel-based classification.\nThe former group of authors also noted that the OBIA results are more spatially consistent than the pixel-based outcomes, which exhibit speckled pixel effects and are dependent on visual interpretation. The latter group stated that topographic features play the most crucial role in improving the overall accuracy of classification in both pixel- and object-based models incorporating all features. Furthermore, higher accuracy is achieved when the object-based method is used with only spectral data, although small objects on the ground cannot be adequately monitored. However, when combined with various types of auxiliary features, the object-based method can identify small objects while also achieving greater accuracy.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>WeekSeven: Classification-2 The Big Question</span>"
    ]
  }
]