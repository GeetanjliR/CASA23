# WeekSix: Classification

## Summary

Re-collecting from last week, GEE provides quick, vast data volume handling, systematic and computer-controlled environment for processing, classification, accuracy improvement etc.for pixel imagery data. In the RS cloud computation, the classification of pixel imagery data happens in two ways, supervised and unsupervised. Where supervised classification needs much control and input details on pixel data classification based on outputs of pattern recognition or machine learning, classifier learns patterns in the data, uses that to place labels onto new data, pattern vector used to classify the image by the analysts. The unsupervised classification is not much interfered such as identification of land cover classes aren't know a prior, requests computer to cluster based on information (e.g. bands), label the clusters, etc. The classification by the machine learning in terms of pattern recognition relating thematic information extraction is also known as intuitive learning. The digital image processing in remote sensing also uses information extraction using AI, such as expert systems, support vector machines, random forest classifier. Finally, the error matrix derives thematic map accuracy assessment using GEE in RS. As an urban spatial science professionals it is important to understand, behind the scene requirements of geospatial cloud computation to be aware and critical of its processes and application in future endeavors.

### Classification and Regression Tree (CART)

Knowing that 'Regression' is also a ML method if finding a line of best fit between the dependent and independent variables, Classification and regression trees (CART) become handy to classify (classification tree) and predict (regression tree) pixel imagery data. Classification tree is useful in pattern recognition on discrete category of data sets such as temperature, rainfall, wind, saturation where as regression tree for continuous dependent variables eg. GCSE. However, the linear regression does not fit, so sub-setting the data (with means of decision tree having root, branch and leaves) into smaller chunks seems doable known as regression tree. These decision tree may go deeper, so a Gini impurity finding the lowest impurity wins. Additionally, in the interest of numerical value, dividing the sections based on thresholds (nodes) and calculate the sum of the squared residuals (SSR) to check the SSR for different thresholds. Setting a minimum number of observations to prevent over fitting, splitting with the lowest SSR root of the tree (starting with) is suggested. The process is repeated across all variable (as root), gives a numeric value unlike category in classification trees.To avoid over fitting, best model holds low bias and low variability (e.g. test and train) may be done through limiting the growth of tree (minimum number of pixels in a leaf, 20 is often) or pruning by identifying the weakest link. The challenge of decision trees with new data makes to use random forest method more appropriate.

Figure-2: Classification and Regression Tree (CART)

```{r echo=FALSE, out.width='80%', fig.align='center'}

knitr::include_graphics('img/CART.jpg')

```

https://nature.com/articles/nmeth.4370

### Random Forest

Random forest means having more than one decision tree as many are better than one. Here, the concept of bootstrap samples get implemented, where re-sampling gets done by replacing data to make a decision as 'bagging'. Thus, about 70% of the 'training' data is used in the bootstrap and remaining 30% is left known as out of bag (OOB) samples are the 'testing' data, carried for each tree which is repeated for all OOB samples. It is evident that no pruning is required as voting wins. Lastly, a remaining proportion of OOB incorrectly classified is called out of bag error (OOBE). However, there exists the problem of validation data which is different from OOB and never gets included within the decision trees.

Figure-1 Random Forest Analysis

```{r echo=FALSE, out.width='80%', fig.align='center'}

knitr::include_graphics('img/RF.jpg')

```

<https://corporatefinanceinstitute.com/resources/data-science/random-forest/>

### Working with Imagery

Insights about the know-how of supervised and unsupervised classification on pixel data in RS, it becomes imperative to learn its working on imagery. As such machine learning has advanced much, there are generic machine learning algorithms and there are those specific to remote sensing. Under supervised classification, usually pixels are treated in isolation but more as contextual (neighboring pixels), objects (polygons), texture. While, in unsupervised classification operates on clustering also known as k-means, DBSCAN algorithms. Next, the iterative self-organizing data analysis technique (ISODATA) algorithm used for multispectral pattern recognition does cluster busting. Besides, the algorithms learnt i.e., decision trees and random forests; maximum likelihood and support vector machines are two common methods significant to supervised classification. The maximum likehood is a traditional classifier where a prior probability information e.g. 60% urban is specified to the data (landcover) most probably to have the values in the pixel, which also fuse data. On the other hand, the support vector machine (SVM) algorithm, set a SVM hyperplane betweem the maximum margin classifier of support vectors. Additionally, 3D and use of a plane supports the process for more than 2 datasets.

## Related Literature Findings

The research by Chen, W. et al. carried a comparative assessment of logistic model tree, random forest, and classification and regression tree models for spatial prediction of landslide susceptibility. They found that random forest model showed promising application with success rate of 0.837 and a prediction rate of 0.781 than CART incase of landslide susceptibility mapping in Long County, Georgia. However, a study by team of Mohajane, M. also found the RF (RF-FR) achieved the highest performance (AUC = 0.989) that out performed other methods but interestingly established SVM (AUC = 0.959) to be next supportive after RF, better than CART (AUC = 0.847) in forecasting of the forest fire. The former study applies linear-support vector machine algorithm (L-SVM) to evaluate the predictive capability of the 12 landslide conditioning factors. Whereas, the later researches about the developes Frequency Ratio-Support Vector Machine (FR-SVM) model as one of the hybrid machine learning algorithms to compare with other four i.e., Frequency Ratio-Multilayer Perceptron (FR-MLP), Frequency Ratio-Logistic Regression (FR-LR), Frequency Ratio-Classification and Regression Tree (FR-CART) and Frequency Ratio-Random Forest (FR-RF), for mapping forest fire susceptibility in the north of Morocco. The first study considered twelve landslide-related parameters, including slope angle, slope aspect, plan curvature, profile curvature, altitude, NDVI, land use, distance to faults, distance to roads, distance to rivers, lithology, and rainfall. On the other hand, the second study used 10 independent causal factors including elevation, slope, aspect, distance to roads, distance to residential areas, land use, normalized difference vegetation index (NDVI), rainfall, temperature, and wind speed.

## Reflection

Although the lecture was long but it was worth the effort to learn the cool stuffs that happens behind the scene for GEE application in RS. I liked the opening part to relate how the supervised classification of CART and random forest method that drives ahead the machine learning pixel imagery processing through a intuitive learning method of human way. Thus, the jargon such as terms Machine Learning, Cloud Computing, Deep Learning etc. that used to creep me out, has now become a fun learning, exploring and application. Additional, it was so relieving when the professor told us, there are only few bits of codes that can do everything we covered in 84 slides of lecture today that will take all our pain away. Moreover, trying to keep-up in this long lecture made much sense when we related to the concepts of CASA0005 about the linear regression, best line-fit, residual assumption, clustering, k-means, DBSCAN. I was little skeptical about not being in practice with GIS concepts or R-studio but everything just fell into place.

While pixel imagery data processing get done by classification of supervised and unsupervised algorithms, I am intrigued that the identification and processing of hard or soft surface raises question if we use pixel or object method, which is coming soon... Yeyyi!

Reference: Chen, W., Xie, X., Wang, J., Pradhan, B., Hong, H., Bui, D.T., Duan, Z. and Ma, J., 2017. A comparative study of logistic model tree, random forest, and classification and regression tree models for spatial prediction of landslide susceptibility. Catena, 151, pp.147-160.

Mohajane, M., Costache, R., Karimi, F., Pham, Q.B., Essahlaoui, A., Nguyen, H., Laneve, G. and Oudija, F., 2021. Application of remote sensing and machine learning algorithms for forest fire mapping in a Mediterranean area. Ecological Indicators, 129, p.107869.
